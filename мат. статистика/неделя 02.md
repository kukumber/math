
# параметрическая модель

Пусть у нас имеется вероятностно-статистическая модель $(\mathcal{X}, \mathcal{B_X}, \mathcal{P})$

$\mathcal{X}$ — это пространство значений эксперимента
$\mathcal{B_X}$ — борелевская σ-алгебра на нем
$\mathcal{P}$ — множество всех возможных распределений для данной модели

Если $\mathcal{P}$ может быть представлена в следующем виде
$$\mathcal{P}=\{P_\theta, \theta\in\Theta\}$$
где обязательное условие $\Theta\subset \mathbb{R}$ или $\Theta\subset \mathbb{R}^n$

В таком случае можно говорить о *параметрической модели* 
$$(\mathcal{X}, \mathcal{B_X}, \{P_\theta,\theta\in\Theta\})$$
Параметрическая модель нужна потому, что оценивать вероятностную меру напрямую, не предполагая каких-то условий на то, что вероятностная мера, то есть распределение, принадлежит некоему классу распределний довольно тяжело. В случае, когда можно уменьшить семейство вероятностных мер $P$  до какого-то параметрического семейства, задача оценивания истинного распределения по сути сводится к оценке параметра этого распределения внутри параметрической модели, что гораздо проще.

## дискретные параметрические модели

### биномиальное распределение

Семейство биномиальных распределений параметризовано двумя параметрами $m$ и $p$
$$Bin(m,p)$$
где $m\in\mathbb{N}$ и $p\in (0,1)$. Можно определить $p$ также для отрезка, включающего границы $[0,1]$, однако в этом нет смысла, потому что для 0 и 1 получатся вырожденные распределения.
$$P(\xi=k)=C_m^kp^k(1-p)^{m-k}$$
где $k=0,\ldots, m$

### распределение Пуассона

Семейство распределений Пуассона
$$Pois(\lambda)$$
как правило параметризуется только одним параметром $\lambda>0$. В данном случае множество $\Theta$ — это $\mathbb{R}_+$
$$P(\xi=k)=\frac{\lambda^ke^{-\lambda}}{k!}$$
где $k\in\mathbb{Z}_+$ (все натуральные числа и ноль)

### геометрическое распределение

Геометрическое распределение
$$Geom(p)$$
параметризовано одним параметром $p\in(0,1)$
$$P(\xi=k)=p(1-p)^{k-1}$$
где $k \in \mathbb{N}$

## абсолютно непрерывные параметрические модели

### нормальное распределение

Нормальное распределение
$$\mathcal{N}(a,\sigma^2)$$
параметр $a\in(-\infty, +\infty)$  и $\sigma>0$ (0 отбрасываем, потому что в случае, когда $\sigma=0$, случайная величина почти наверное будет равна константе), тогда $\Theta=\mathbb{R}\times\mathbb{R}_+$
$$p(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-a)^2}{2\sigma^2}}$$
где $x\in\mathbb{R}$

### экспоненциальное распределение

Экспоненциальное распределение как правило параметризуется одним параметром $\alpha$, но иногда добавляется параметр $\beta$, который является параметром сдвига
$$Exp(\alpha)$$
параметр $\alpha>0$, тогда $\Theta=\mathbb{R}_+$
$$p(x)=\alpha e^{-\alpha x} I(x>0)$$
### гамма распределение

Гамма распределение параметризовано $\alpha$ и $\beta$
$$\Gamma(\alpha, \beta)$$
параметры $\alpha >0$, $\beta> 0$
$$p(x)=\frac{x^{\alpha-1}\beta^\alpha}{\Gamma(\alpha)}e^{-\beta x}I(x>0)$$
В википедии параметры $\alpha$ и $\beta$ меняются местами. Плотность можно определять так, потому что, если взять независимые одинаково распределенные случайные величины, распределенные по экспоненциальному закону, допустим, с параметром $\alpha$ и сложим их, в таком случае получится гамма распределение с параметрами $n$ и $\alpha$. 

Гамма-функция в знаменателе
$$\Gamma(\alpha)=\int_0^{+\infty}x^{\alpha-1}e^xdx$$
В большинстве случае этот интеграл посчитать сложно.

### бета распределение

$$Beta(\alpha,\beta)$$
где параметры $\alpha>0$, $\beta>0$. В данном случае $\Theta=\mathbb{R}_+\times\mathbb{R}_+$
$$p(x)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}I\big(x\in[0,1]\big)$$
где бета-функция
$$B(\alpha,\beta)=\int_0^1x^{\alpha-1}(1-x)^{\beta-1}dx$$

### распределение Коши

Как правило семейство распределений Коши параметризовано одним параметром $\theta$, но иногда еще добавляется параметр сдвига
$$Cauchy(\theta)$$
где $\theta>0$
$$p(x)=\frac{\theta}{\pi(\theta^2+x^2)}$$
где $x\in\mathbb{R}$.

Для любого распределения из семейства распределений Коши не существует математического ожидания.

### распределение $\chi^2$ 

Распределение $\chi^2$ с $n$ степенями свободы
$$\chi^2_n$$
где $n\in\mathbb{R}$

Распределение $\chi^2$ — это частный случай гамма распределения
$$\chi^2_n=\Gamma\bigg(\frac{n}{2},\frac{1}{2}\bigg)$$

# выборочные характеристики

Пусть $X$  — наблюдение на выборочном пространстве $\mathcal{X}$. Тогда отображение 
$$S:(\mathcal{X},\mathcal{B}_{\mathcal{X}})\to (Y,\mathcal{E})$$
называется статистикой, если это отображение является измеримым.

Если $S$ имеет своими значениями $\Theta$, то есть действует из $\mathcal{X}$ в $\Theta$, то статистика $S(\mathcal{X})$ будет называться *оценкой параметра* $\Theta$.

## виды статистик

### выборочные характеристики

Пусть есть выборка $X=(X_1,\ldots, X_n)$, то есть последовательность независимых одинаково распределенных случайных величин, из распределения $P$. Пусть есть борелевская функция $g(x)$. Тогда статистика
$$\overline{g(X)}=\frac{1}{n}\sum_{i=1}^ng(X_i)$$
называется *выборочной характеристикой* функции $g(x)$.

#### выборочное среднее

$$\overline{X}=\frac{1}{n}\sum_{i=1}^nX_i$$
Если существует математическое ожидание, то эта статистика по закону больших чисел сходится именно к нему.

#### выборочный k-й момент

$$\overline{X^k}=\frac{1}{n}\sum_{i=1}^nX_i^k$$
Если существует математическое ожидание $X^k$, то эта статистика по закону больших чисел сходится именно к нему.

### функции от выборочных характеристик

Пусть есть борелевские функции $h(x)$, $g_1(x),\ldots,g_k(x)$. Тогда следующее является статистикой
$$S(X)=h\big(\overline{g_1(X)},\ldots,\overline{g_k(X)}\big)$$

#### выборочная дисперсия
$$s^2=\overline{X^2}-(\overline{X})^2$$

#### k-й выборочный центральный момент

$$M_k=\frac{1}{n}\sum_{i=1}^n(X_i-\overline{X})^k$$

Выборочная дисперсия — это второй выборочный центральный момент.
$$s^2=\frac{1}{n}\sum_{i=1}^n(X_i-\overline{X})^2=$$
$$=\frac{1}{n}\sum_{i=1}^nX_i^2-2\frac{1}{n}\sum_{i=1}^nX_i\overline{X}+(\overline{X})^2=$$
$$=\overline{X^2}-2\overline{X}\overline{X}+(\overline{X})^2 =\overline{X^2}-(\overline{X})^2$$

### выборочные характеристики

Есть выборка $X_1,\ldots, X_n$, тогда обозначим первый член вариационного ряда как минимум из всех значений выборки
$$X_{(1)}=\min(X_1,\ldots,X_n)$$
Это первая порядковая статистика или первый член вариационного ряда.

Вторая порядковая статистика — это минимум из всех значений без учета первой порядковой статистики
$$X_{(2)}=\min\{(X_1,\ldots,X_n)\backslash X_{(1)}\}$$
Последняя порядковая статистика
$$X_{(n)}=\max(X_1,\ldots,X_n)$$

С помощью этих статистик можно определить **выборочную квантиль**.

Пусть $F(x)$ — это функция распределения, тогда $p$-квантилью этого распределения называется такое значение $X_p$, что выполнено следующее соотношение
$$X_p=\inf\{x:F(X)\ge p\}$$
Выборочная квантиль уровня $p$ — это 
$$\hat{Z}_p=
\begin{cases}
X_{(np)}, & np\in\mathbb{N}\\
X_{[np]+1}, & np \notin\mathbb{N}
\end{cases}$$

Выборочная медиана определяется как $1/2$ выборочная квантиль.

# свойства оценок

## несмещенность

Пусть $X$ — наблюдение из неизвестного распределения $P\in\{P_\theta,\theta \in \Theta\}$. Тогда оценка $\hat{\theta}(X)$ называется несмещенной оценкой параметра $\theta$, если для любого $\theta\in \Theta$ выполнено следующее соотношение
$$E_\theta\hat{\theta}(X)=\theta$$
Пусть $\theta$ зафиксировано и пусть наблюдение $X$ из распределения $P_\theta$, в таком случае $E\hat{\theta}(X)=\theta$.
$$E_\theta\hat{\theta}(X)=\int \hat{\theta}(x)dP_\theta$$
Например, пусть $X_1,\ldots, X_n\sim\mathcal{N}(a, \sigma^2)$. Требуется предложить несмещенную оценку параметра $a$.

Зафиксируем параметр $a$ и найдем математическое ожидание 
$$E_a\overline{X}=\frac{1}{n}\sum_{i=1}^nE_aX_i=\frac{1}{n}\sum_{i=1}^na = a$$
## состоятельность

Пусть $X_1, \ldots, X_n$ — выборка. Тогда $\hat{\theta}_n(X_1, \ldots, X_n)$ — последовательность оценок или оценка, является **состоятельной оценкой** параметра $\theta$, если выполнено 
$$\forall\theta\in\Theta \ \hat{\theta}_n(X_1, \ldots, X_n)\xrightarrow[n\to\infty]{P_\theta}\theta$$
Предположим, что $X_1, \ldots, X_n$ из распределения $P_\theta$, в таком случае такая оценка по вероятности будет сходиться к $\theta$.

Оценка $\hat{\theta}_n$ называется **сильно состоятельной оценкой** $\theta$, если
$$\forall\theta\in\Theta \ \hat{\theta}_n(X_1, \ldots, X_n)\xrightarrow[n\to\infty]
{P_\theta \ п.н.}\theta$$

Смысл состоятельности в том, что мера множества элементарных исходов, на которых значение оценки $\hat{\theta}$ отличается от неизвестного параметра $\theta$ на некую наперед заданную величину $\varepsilon$ с ростом выборки уменьшается. Чем больше размер выборки, тем ничтожнее вероятность того, что произойдет ошибка при оценивании при помощи $\hat{\theta}$.


## асимптотическая нормальность

Оценка $\hat{\theta}_n(X_1, \ldots, X_n)$ является асимптотически нормальной оценкой парметра $\theta$
$$\forall\theta\in\Theta \ \sqrt{n}(\hat{\theta}_n-\theta)\xrightarrow[n\to\infty]{d_\theta}\mathcal{N}(0,\sigma^2(\theta))$$
У нормального закона должен быть сдвиг 0. 

$\sigma^2$  называется асимптотической дисперсией $\hat{\theta}_n$.

# свойства k-того выборочного момента

**Задача**

Доказать, что выборочный $k$-й момент является несмещенной, состоятельной и асимптотически нормальной оценкой параметра $E_\theta X_1^k$, при условии, что $E_\theta X_2^k< +\infty$ .
$$E_\theta \overline{X^k}=\frac{1}{n}\sum_{i=1}^nE_\theta X_i^k=$$
поскольку у нас выборка, каждое матожидание будет равняться одному и тому же, потому что члены выборки одинаково распределены
$$=\frac{1}{n}\sum_{i=1}^nE_\theta X_1^k=E_\theta X_1^k$$
далее можно воспользоваться законом больших чисел
$$\overline{X^k}=\frac{1}{n}\sum_{i=1}^nE_\theta X_i^k\xrightarrow[]{P_\theta}E_\theta X_1^k$$
воспользуемся центральной предельной теоремой
$$\sqrt{n}(\overline{X^k}-E_\theta X_1^k)\xrightarrow[n\to\infty]{d_\theta}\mathcal{N}(0,D_\theta X_1^k)$$

# свойства выборочной дисперсии

**Задача**

Доказать, что выборочная дисперсия  $s^2$ является состоятельной оценкой параметра $\tau(\theta)=D_\theta X_1$ .
Является ли $s^2$ несмещенной оценкой данного параметра?

По определению выборочная дисперсия равна выборочному квадрату минус квадрат выборочного среднего
$$s^2 = \overline{X^2}-(\overline{X})^2$$Для сходимости по вероятности можно пользоваться свойством: если есть сходимость по вероятности компонент случайного вектора, то и сам вектор сходится по вероятности. Значит по теореме о наследовании сходимости можно брать произвольные функции.

Мы знаем, что $\overline{X^2} \xrightarrow[]{P_\theta} E_\theta X_1^2$,  а $\overline{X} \xrightarrow[]{P_\theta} E_\theta X_1$. В таком случае
$$s^2 = \overline{X^2}-(\overline{X^2})\xrightarrow[]{P_\theta}E_\theta X_1^2-(E_\theta X_1)^2=D_\theta X_1$$
Осталось доказать, что $s^2$ — это смещенная оценка.
$$\begin{aligned}
E_\theta s^2&=E_\theta\overline{X^2}-(E_\theta\overline{X})^2= \\ &=E_\theta X_1^2-D_\theta\overline{X}-(E_\theta\overline{X})^2=\\
&=E\theta X_1^2-(E\theta X_1)^2-D_\theta \overline{X}= \\
&=DX_1-\frac{1}{n^2}\sum_{i=1}^nD_\theta X_i = \\
&=DX_1-\frac{1}{n^2}DX_1 = \frac{n-1}{n}DX_1
\end{aligned}$$
Часто для того, чтобы оценка $s^2$ была несмещенной рассматривают величину $s^2\cdot\frac{n}{n-1}$

# наследование свойств

Пусть $\hat{\theta}$ — это асимптотически нормальная оценка параметра $\theta$ с асимптотической дисперсией $\sigma^2(\theta)$. Пусть $\tau(\theta)$ является дифференцируемой на $\Theta$. Тогда $\tau(\hat{\theta}_n)$ является асимптотически нормальной оценкой параметра $\tau(\theta)$ с асимптотической дисперсией $\big(\tau'(\theta)\big)^2\cdot\sigma^2(\theta)$.