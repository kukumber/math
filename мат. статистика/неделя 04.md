# функция потерь, функция риска

Допустим, есть ситуация, когда есть несколько оценок и требуется понять, какая из них лучше. В статистике существует несколько подходов к сравнению оценок, например, равномерный, минимаксный, асимптотический и байесовский.

**Функция потерь** — это функция $g(x,y)$, тогда, когда $g(x,y)>0$ если $x\ne y$, и $g(x,x)=0$.

**Функция риска.** 
Пусть есть некоторое некоторое наблюдение $X$ из распределения $P$, которое принадлежит некоторому параметрическому семейству распределений
$$X\sim P\in \{P_\theta,\theta\in\Theta\}$$
Пусть есть некоторая функция потерь $g(x,y)$, и есть $\hat{\theta}$ — оценка параметра $\theta$. Тогда **функцией риска**, связанной с оценкой $\hat\theta$ называется выражение
$$R(\hat{\theta},\theta)=E_\theta\big(g(\hat\theta,\theta)\big)$$
## равномерный подход

Пусть есть две оценки $\hat\theta$ и $\tilde\theta$ параметра $\theta$, тогда оценка $\hat\theta$ называется лучшей в рамномерном подходе, чем оценка $\tilde\theta$, если $\forall\theta\in\Theta$ выполнено
$$R(\tilde\theta, \hat\theta)\ge R(\hat\theta, \theta)$$

Если оценка $\hat\theta$ лучше всех оценок $\tilde\theta$, которые принадлежат некоторому классу оценок $\tilde\theta\in \mathcal{K}$, то $\hat\theta$ называется **наилучшей** в классе $\mathcal{K}$.

Если равномерный подход с квадратичной функцией потерь, то он будет называться среднеквадратическим подходом.

### пример

Есть выборка $X_1,\ldots,X_n$ из равномерного закона на отрезке $R[0,\theta]$. Найти наилучшую оценку в рамках среднеквадратического подхода в классе оценок 
$$\mathcal{K}=\{cX_{(1)},c\in\mathbb{R}\}$$

Необходимо минимизировать функцию риска 
$$E_\theta(cX_{(1)}-\theta)^2\to\min_c$$
найдем плотность первого члена вариационного ряда, для этого нужно найти функцию распределения
$$\begin{aligned}
F_{X_{(1)}}(x)&=P(X_{(1)}\le x)=1-P(X_{(1)} > x)=1-\bigg(1-\frac{x}{\theta}\bigg)^n\Rightarrow\\
&\Rightarrow p_{X_{(1)}}(x)=\frac{n(\theta-x)^{n-1}}{\theta^n}
\end{aligned}$$
Теперь найдем матожидание $X_{(1)}$
$$\begin{aligned}
E_\theta X_{(1)}=\int_0^\theta x \frac{n(\theta-x)^{n-1}}{\theta^n}dx=\frac{\theta}{n+1}
\end{aligned}$$
Теперь найдем матожидание $X_{(1)}^2$
$$\begin{aligned}
E_\theta X_{(1)}^2&=\int_0^\theta x^2 \frac{n(\theta-x)^{n-1}}{\theta^n}dx=
\end{aligned}$$
возьмем переменную $y=\frac{x}{\theta}$
$$\begin{aligned}
&=\int_0^1 (y\theta)^2 n(1-y)^{n-1}dy= \\
&=\theta^2 n \int_0^1 y^2 (1-y)^{n-1}dy= \\
&=\theta^2 n B(3,n)=\\
&=\theta^2 n \frac{\Gamma(3)\Gamma(n)}{\Gamma(n+2)}=\\
&=\theta^2 n\frac{2(n-1)!}{(n+2)!}= \frac{2\theta^2}{(n+1)(n+2)}
\end{aligned}$$

вернемся к матожиданию
$$\begin{aligned}
E_\theta (cX_{(1)}-\theta)^2&=c^2 \frac{2\theta^2}{(n+1)(n+2)}-2c\theta\frac{\theta}{n+1}+\theta^2=\\
&=\theta^2\bigg(c^2\frac{2}{(n+1)(n+2)}-\frac{2c}{n+1}+1\bigg)
\end{aligned}$$
минимум квадратичной функции достигается в точке $\frac{-b}{2a}$, где $b$ — это коэффициент перед вторым членом, $a$ — коэффициент перед $c^2$.
$$c_{\min}=\frac{\frac{2}{(n+1)}}{\frac{4}{(n+1)(n+2)}}=\frac{n+2}{2}$$

# условия регулярности для неравенства Крамера-Рао

Неравенство Крамера-Рао при некоторых условиях регулярности устанавливает нижнюю границу для дисперсии несмещенной оценки. Это означает, что невозможно бесконечно много раз улучшать оценку при условии, что набор данных ограничен. 

## условия регулярности

1. $\Theta$ — открытый интервал на действительной прямой. Может быть бесконечным.
   
2. Введем множество $\mathcal{A}=\{x:p_\theta(x)>0\}$ — носитель плотности. Носитель не зависит от параметра $\theta$.
   
   Контрпример — равномерное распределение с неизвестным правым концом $R(0,\theta)$.
   
3. Пусть есть статистика $S(X)$, тогда интеграл 
$$\int_{\mathcal{A}}S(x)\frac{\partial}{\partial\theta}p_\theta(x)d\mu=\frac{\partial}{\partial\theta}\int_{\mathcal{A}}S(x)p_\theta(x)d\mu$$

4. Информация Фишера положительна и существует
$$I_\theta(X)=E_\theta\bigg(\frac{\partial}{\partial\theta}\ln f_\theta(X)\bigg)^2$$
предполагается, что правдоподобие существует
$$0<I_\theta(X)<+\infty, \forall\theta\in \Theta$$

## теорема неравенство Крамера-Рао

Пусть $X$ — наблюдение из неизвестного распределения $P\in\{P_\theta,\theta\in\Theta\}$ и для этого семейства $P_\theta$ выполнены условия регулярности. Кроме того $\hat\theta(X)$ — несмещенная оценка параметра $\tau(\theta)$. Тогда верно следующее неравенство
$$D_\theta\hat\theta(X)\ge\frac{\big(\tau'(\theta)\big)^2}{I_\theta(X)}$$

Если для оценки $\hat\theta(X)$ в неравенстве Крамера-Рао достигается равенство, то такая оценка называется **эффективной** оценкой параметра $\tau(\theta)$.

## свойства информации Фишера

Если в качестве наблюдения $X$ взять выборку $X_1,\ldots,X_n$, то для информация Фишера распадается на $n$ информаций Фишера
$$I_\theta(X)=n\cdot i(\theta)$$
где $i(\theta)$ — это информация, скрытая в одном наблюдении.

$$E_\theta\frac{\partial}{\partial\theta}f_\theta(X)=E_\theta\frac{\partial}{\partial\theta}\ln\prod_{i=1}^np_\theta(X_i)=$$
поскольку $X_i$ являются независимыми, то матожидание от произведения — это произведение матожиданий
$$=\prod_{i=1}^nE_\theta\frac{\partial}{\partial\theta}\ln p_\theta(X_i)$$
найдем одно ожидание
$$E_\theta\frac{\partial}{\partial\theta}p_\theta(X_1)=\int_\mathcal{A}\frac{\partial}{\partial\theta}\ln p_\theta(x)p_\theta d\mu=$$
если продифференцировать логарифм от плотности, то получится производная от плотности деленная на плотность
$$=\int_{\mathcal{A}}\frac{\frac{\partial}{\partial\theta}p_\theta(x)}{p_\theta(x)}p_\theta(x)d\mu=\int_{\mathcal{A}}\frac{\partial}{\partial\theta}p_\theta(x)d\mu=$$
тогда по третьему условию регулярности можно вынести из-под интеграла дифференециал
$$=\frac{\partial}{\partial\theta}\int_{\mathcal{A}}p_\theta(x)d\mu=$$
интеграл от плотности по всему множеству $\mathbb{R}$ равен единице, так как здесь интегрирование по множеству $\mathcal{A}$, то неважно интегрирование проводится по $\mathbb{R}$ или по $\mathcal{A}$. Получается, что необходимо продифференцировать единицу
$$=\frac{\partial}{\partial\theta}1=0$$
Найдем информацию Фишера
$$E_\theta\bigg(\frac{\partial}{\partial\theta}\ln \prod_{i=1}^np_\theta(X_i)\bigg)^2=$$
можем записать это как сумму
$$=D_\theta\bigg(\frac{\partial}{\partial\theta}\sum_{i=1}^n\ln p_\theta(X_i)\bigg)=$$
дифференциал можно внести внутрь суммы
$$=D_\theta\bigg(\sum_{i=1}^n\frac{\partial}{\partial\theta} \ln p_\theta(X_i)\bigg)=$$
дисперсия суммы независимых величин равна сумме дисперсий
$$=\sum_{i=1}^nD_\theta\bigg(\frac{\partial}{\partial\theta}\ln p_\theta(X_\theta)\bigg)=$$
вернемся к матожиданию
$$=\sum_{i=1}^n E_\theta\bigg(\frac{\partial}{\partial\theta}\ln p_\theta(X_i)\bigg)^2=n\cdot i(\theta)$$

# эффективные оценки

## Критерий эффективности

Оценка $\hat\theta(X)$ параметра $\tau(\theta)$ является **эффективной** тогда и только тогда, когда
$$\hat\theta(X)-\tau(\theta)=c(\theta)\cdot\frac{\partial}{\partial\theta}\ln f_\theta(X)$$
где 
$$c(\theta)=\frac{\tau'(\theta)}{I_\theta(X)}$$

## задача

Пусть есть выборка из распределения Бернулли $X_1,\ldots,X_n\sim Bern(\theta)$. Найти эффективную оценку $\theta$ и информацию Фишера одного наблюдения.

Функция правдоподобия
$$f_\theta(X)=\theta^{\sum X_i}(1-\theta)^{n-\sum X_i}$$
логарифмическая сумма правдоподобия
$$L_\theta(X)=\sum X_i \ln\theta+(n-\sum X_i)\ln(1-\theta)$$
при дифферинцировании получится
$$\frac{\partial}{\partial\theta}L_\theta(X)=\frac{\sum X_i}{\theta}-\frac{n-\sum X_i}{1-\theta}=$$
вынесем за скобки 
$$=\frac{n}{\theta(1-\theta)}(\overline X(1-\theta)-\theta(1-\overline X))=$$
после сокращения
$$=\frac{n}{\theta(1-\theta)}(\overline X- \theta)$$

в соответствии с критерием эффективности
$$\frac{1}{\theta(1-\theta)}=\frac{I_\theta(X)}{\tau'(\theta)}=I_\theta(X)$$
и тогда информация, скрытая в одном наблюдении
$$i(\theta)=\frac{1}{\theta(1-\theta)}$$
---
Эффективная оценка существует только для одного параметра $\theta$.

Если $\hat\theta(X)$ — эффективная оценка $\tau(\theta)$, то оказывается, что $(a\hat\theta(X)+b)$ — это эффективная оценка $(a\tau(\theta)+b)$.