
# Постановка задачи

Первое, о чем мы будем говорить, — это про то, как ставить задачи оптимизации. То есть, как определить целевую функцию, что такое допустимое множество, функция ограничений, как это все записывается правильно и дальше, как это можно анализировать.
 
После этого мы посмотрим на примеры прикладных задач, которые будут уже в этой лекции, и как эти задачи сводятся к задачам оптимизации.

После этого мы начнем изучение теории, в частности, мы посмотрим, что такое *выпуклое множество* и *выпуклая функция*, и как эти понятия связаны с задачами выпуклой оптимизации, и каким приятным свойством, в отличие от произвольной задачи оптимизации, они обладают.

После этого мы посмотрим, что многие из рассмотренных нами ранее задач являлись на самом деле не просто задачами *оптимизации*, но *выпуклой оптимизации*, и, соответственно этот факт дает нам инструмент и некоторую дополнительную информацию о том, как искать решения и почему найденная точка будет решением.

Следующий момент  — это *условия оптимальности* и то, как их применять. Это те условия, которые позволяют определить, является ли найденная нами точка минимумом или нет, и если нет, то почему, и как можно было бы модифицировать наш метод, чтобы эта точка стала минимумом.

В конце мы рассмотрим *теорию двойственности*. Это очень важный элемент теории выпуклой оптимизации, которые позволяет вам строить нижние оценки для оптимального значения целевой функции. Таким образом, даже для NP-сложных задач и, в частности, задач дискретной оптимизации вы можете получить выпуклую задачу оптимизации, решение которой, однако, даст вам только нижнюю границу решения исходной задачи. В то же время существуют некоторые теоремы и условия, при которых решение двойственной задачи, про которые мы будем в конце говорить, будет совпадать с решением прямой задачи. Это очень важный факт в теории и на практике, потому что он позволяет вам находить одновременно и решение прямой задачи, и решение двойственной задачи, и находить между ними связь и то, как связаны точки минимума. Не сами значения целевых функций, которые будут совпадать, но точки минмума, на которых они достигаются. Эти факты очень активно используются именно в методах оптимизации, про которые будет идти речь в следующем семестре, однако, некоторую теорию и основные факты об этих объектах, то есть двойственной задаче и двойственной функции, мы обсудим в конце этого семестра. 

# Общая методология

Общая методология преобразования задачи из реального мира в задачу оптимизации. 

Основные этапы использования методов оптимизации при решении реальных задач:
1. определение целевой функции
2. определение допустимого множества решений
3. постановка и анализ оптимизационной задачи
4. выбор наилучшего алгоритма для решения поставленной задачи
5. реализация алгоритма и проверка его корректности

Перечислим некоторые этапы, которые необходимо сделать, чтобы перевести какую-то задачу неформальную в задачу формальную. 

Для этого необходимо, первое, определить целевую функцию, то есть понять из каких-то соображений, присущих предметной области, то, какая функция отражает качества тех элементов, которые вам нужно найти.

Далее нужно определиться с множеством, из которого эти элементы могут быть. Например, это могут быть векторы с неотрицательными значениями или векторы, сумма элементов которых равна единице, или это могут быть положительно определенные матрицы, или это могут быть элементы некоторого линейного подпространства, то есть, решения некоторой недоопределенной системы линейных уравнений и т. д.

После этого вы записываете то, что вы зафиксировали в первых двух пунктах, объядиняя все это вместе ставите задачу оптимизации. Тем самым, сводя всю имеющуюся у вас информацию и то, что вы хотите получить, вместе.

После чего вам необходимо определить какой алгоритм решения этой задачи является лучшим. В этом курсе мы обсудим то, какими свойствами должна обладать задача, чтобы ее можно было эффективно решить, а также, как можно из задачи, которая этими свойствами не обладает, получить задачу, которая этими свойствами обладает и, соответствует некоторому приближению решений исходной задачи.

Дальше вы реализуете алгоритм, который вы выбрали и проверяете его корректность. То, что вы его реализовали правильно, а также то, что он выдает то решение, которое ожидается. То есть, что ваша целевая функция и допустимое множество не являются противоречивыми, или, что допустимое множество не пусто. Также смотрите на то, можно ли как-то ускорить работу алгоритма, для чего можно рассмотреть, например, либо другую целевую функцию, либо другое допустимое множество, которое не совсем отражает ваши идеальные желания, но дает некоторое приближение к желаемой точности решения, но, возможно, будет решаться более быстро, нежели исходная постановка задачи. 

Происходит цикл между пунктами 1 и 5. 

Пункт 3 про постановку и реализацию задачи формулируется следующим образом: вы минимизируете некоторую целевую функцию $\min_{x\in X}f_0(x)$ по аргументам, которые лежат из множества $X$, которое не определяется в качестве ограничений типа равенств и неравенств. А ограничения типа равенств и неравенств задаются следующими функциями
$$f_i(x)=0,i=1,\ldots,p$$
$$f_j(x)\le 0, j=p+1,\ldots,m$$

$x\in\mathbb{R}^n$ — искомый вектор
$f_0(x): \mathbb{R}^n\to\mathbb{R}$ — целевая функция
$f_k(x): \mathbb{R}^n\to\mathbb{R}$ — функции ограничений

Это общая запись задачи оптимизации, которая может быть как выпуклой, так и невыпуклой. Таким же образом можно сформулировать задачи дискретной оптимизации, где $x$ будут лежать либо в множестве действительных, либо в множестве натуральных чисел или векторы из натуральных чисел и т. д.

Когда выбраны целевая функция и допустимое множество, следует записать задачу именно в такой форме, то есть минимизируется некоторая функция $f_0$, при условии, что $x$ лежит в каком-то множестве, что выполнены какие-то ограничения типа равенств $f_i(x)=0$ и ограничения типа неравенств $f_j(x)\le 0$. После этого начинается анализ задачи и выяснение, является ли функция выпуклой, является ли допустимое множество выпуклым.

# Определения

**Точкой глобального минимума** называется точка $x^*$, если целевая функция в этой точке меньше, чем целевая функция в любой другой точке из области определения функции $x\in dom f_0$
$$f_0(x^*)\le f_0(x)$$
$x$ и $x^*$ должны лежать в допустимой области, то есть в той области, где все ограничения выполнены ($f_i(x^*)=0, f_j(x^*)\le 0$), и $x$ должен лежать в допустимом множестве $X$. 

**Точкой локального минимума** называется точка $x^*$, которая лежит в области определения функции $f_0$ и в допустимом множестве для задачи оптимизации, однако значение функции должно быть меньше не для всех $x$ из области определения, а только для тех $x$, которые лежат в некоторой ε-окрестности $x\in N_{\varepsilon}(x^*)$ 
$$f_0(x^*)\le f_0(x)$$

Точка локального минимума не всегда будет точкой локального минимума. 

Если для выпуклых функций найдена точка локального минимума, то автоматически получается точка глобального минимума.

Для функций может существовать локальный минимум, но отсутствовать глобальный минимум. Может быть несколько локальных минимумов, но только один из них является глобальным.

# методы решения задач

В общем случае произвольные задачи оптимизации являются NP-сложными и очень тяжело решаются. Любая задача дискретной оптимизации, NP-полнота которой доказана, может быть сформулирована в форме, указанной выше. Однако, некоторые рандомизированные алгоритмы могут эффективно решать подобные задачи, но важно соблюдать баланс между временем, которое требуется рандомизированному алгоритму для сходимости и устойчивостью этого решения. То есть, в зависимости от начального приближения, от рандомизации внутри перехода из одной точки к другой и т. д.

Определенные классы задач могут быть решены быстро и эффективно:
- линейное программирование
- метод наименьших квадратов
- выпуклая оптимизация

Под эффективным и быстрым решением подразумевается, что существуют алгоритмы, которые работают полиномиально от размера задачи. То есть, в случае NP-полных задач рост экспоненциальный с ростом размерности, а здесь — полиномиальный. Это не означает, что задачи будут эффективно решаться на практике за разумное время, однако теоретические оценки показывают, что они хотя бы решаются полиномиально и для больших размерностей рост будет не таким огромным, как в случае экспоненциального роста для NP-полных задач.

# примеры прикладных задач

## регрессия

Допустим есть некоторая выборка состоящая из векторов $x_i$ и соответсвующих им вещественных чисел $y_i$
$$(x_i, y_i), x_i\in \mathbb{R}^n, y_i\in \mathbb{R}$$
Предполагается, что существует некоторая функция $f$, зависящая как от векторов $x_i$, так и от некоторого неизвестного вектора параметров $w$. Эта фукнция обладает таким свойством, что на элементах $x_i$ она хорошо приближает соответствующие числа $y_i$
$$y_i\approx f(x_i, w)$$
Важно, что эта функция параметрическая, она зависит от некоторого вектора параметров $w$, который чаще всего является вектором из $\mathbb{R}^n$.

Формализуем задачу поиска такого вектора $w$, что параметрическая функция $f(x_i, w)$ хорошо приближает некоторая функция $l$ от остатков, то есть от разности $l(y_i-f(x_i, w)) и принимает минимальное значение
$$\sum_{i=1}^ml\big(y_i-f(x_i,w)\big)\to\min_w$$
Функция $l$ отражает наши пожелания о том, как мы будем мерять разность между $y_i$ и числом, которое возвращает функция.

Можно добавить некоторые ограничения на $w$, например, часто рассматривают только неотрицательные значения или сумма значений должна быть равна единице или какому-то другому фиксированному числу.

### виды $f$

 - линейная
$$f(x,w)=w^Tx$$
В случае линейной функции задача сводится к задаче наименьших квадратов.

- полиномиальная
$$f(x,w)=x_1w_1^2+x_2w_1w_2$$
То есть нелинейно зависит как от $x$, так и от $w$.

- произвольная нелинейная, но гладкая (экспонента, косинус, корень, логарифм и т.д.). 

### виды $l$

- некоторые нормы, как естественная мера величины вектора
$$||x||_2^2=x^Tx$$
- первая норма
$$||x||_1=|x_1|+\ldots+|x_n|$$
- норма бесконечности или чебышёвская норма
$$||x||_{\infty}=\max_{i=1,\ldots,n}|x_i|$$
- некоторая функция схожести
Помимо норм можно рассматривать некоторую функцию схожести, которая следует из предметной области, из которой пришла задача. Не факт, что она будет обладать каким-то свойством норм. 

### пример

Допустим, есть следующее облако точек

![[optimization_01_01.jpg|w50]]

Необходимо построить зависимость $y$ от тех значений $x$, которые есть по оси абсцисс. Если просто посмотреть на это облако точек, то оно может быть похоже на случайный шум и непонятно, какая параметрическая функция может давать такое поведение.

На самом деле это модель затухающих колебаний. Функция нелинейная, не полиномиальная, гладкая
$$f(x|w)=w_1e^{w_2x}\cos(w_3x+w_4)$$
Поэтому, решив задачу оптимизации нелинейных наименьших квадратов, получаем следующую кривую

![[optimization_01_02.jpg|w50]]

Это может быть полезно, например, в том случае, когда облако точек получено из какого-то физического наблюдения.

## задача удаления шума

Рассмотрим задачу на примере зашумленного звукового сигнала, хотя эта же техника может быть применена для постановки задач оптимизации, например, для зашумленного изображения.

Дан зашумленный сигнал $x_c$
$$x_c\in\mathbb{R}^n$$
Необходимо избавиться от шума, то есть построить другой вектор такой же размерности, который содержит ту же полезную информацию, которую содержит вектор $x_c$, однако избавлен от шума и не содержит помех.

Соответственно, можно поставить следующую задачу оптимизации
$$\min_x||x-x_c||_2+\lambda R(x)$$
Поскольку дан только вектор $x_c$, то естественно предположить, что вектор без шумов будет в какой-то мере приближать это $x_c$, потому что $x_c$ был получен из исходного вектора путем прибавления шума. То есть, минимизируем  разность между $x_c$ и неизвестным вектором и добавляем функцию от этого неизвестного вектора, умноженную на константу $\lambda$. Этот подход называется **регуляризацией**. Если бы не было регуляризации, то ответом был бы сам вектор $x_c$.

То есть, $R(x)$ должен каким-то образом отражать некоторые дополнительные свойства, присущие сигналу без шума и которые отсутствуют в сигнале с шумом.

Например, это может быть *гладкость*:
$$R(x)=\sum_{i=1}^n(x_{i+1}-x_i)^2$$
или *отсутствие каких-либо выбросов*:
$$R(x)=\sum_{i=1}^n|x_{i+1}-x_i|$$
В зависимости от того, какая функция $R(x)$ используется, получится немного разное решение с разными свойствами.

Также $\lambda>0$ — это параметр регуляризации, который взвешивает то, насколько сигнал должен быть близок к исходному сигналу или должны быть выполнены указанные свойства.

### пример

Вот, пример зашумленного сигнала

![[optimization_01_03.jpg|w50]]

Если использовать функцию с суммой модулей разностей соседних компонент вектора $x$
$$R(x)=\sum_{i=1}^n|x_{i+1}-x_i|$$
то получится следующий сигнал ($\lambda$ была выбрана около единицы)

![[optimization_01_04.jpg|w50]]

## задача составления портфеля активов

Предположим, есть $n$ активов, доступных для того, чтобы вложить в них деньги. Например, это могут быть акции, ценные бумаги, золото, драгоценности и т.д. 

Известна цена покупки единицы $k$-го актива $p_k^i$ ($i$ — input).

Также есть некоторая модель, которая прогнозирует поведение рынка, из которой можно получить ожидаемую цену продажи единицы $k$-го актива $p^o_k$ ($o$ — output).

Доход от $k$-го актива определяется как
$$r_k=\frac{p^o_k-p^i_k}{p^i_k}$$
Доход может быть как положительный, так и отрицательный.

Задача заключается в том, что необходимо найти долю каждого актива в общем пакете $w_k$.

Поскольку $w_k$ — это доля, то естественное ограничение, что их сумма должна быть равна единице и доля каждого актива должна быть положительной
$$\sum_{i=1}^nw_i=1,w_i\ge0$$
Суммарный доход определяется как сумма произведений каждого актива и доли этого актива в портфеле
$$R=r^Tw$$

Сделаем некоторые модельные предположения. 
- вектор $r$ — нормальный:
$$r\sim\mathcal{N}(\mu,\Sigma)$$
- можно купить произвольную долю актива.
- матожидание суммарного дохода:
$$\mathbb{E}R=\mu^Tw$$
- можно посчитать средний риск:
$$Var(R)=w^T\Sigma w$$
Естественно пожелать, чтобы доход был как можно больше, а риск — как можно меньше. 

Эти предположения обобщаются в следующей постановке задачи. Необходимо ввести некоторый положительный коэффициент $\gamma$, который соответствовал бы балансу между ожиданием по доходности и по риску. Поэтому целевая функция в задаче оптимизации формулируется:
$$\max_w\mu^Tw-\gamma w^T\Sigma w$$
Ограничиваем множество допустимых $w$:
$$\sum_{i=1}^nw_i=1, w_i\ge0$$
Для каждого значения $\gamma$ решение этой задачи будет новое и они каким-то образом будут перетекать друг в друга. Этот параметр в какой-то мере схож с параметром регуляризации.

## задача классификации

Пусть есть выборка $(x_i,y_i)$, каждому вектору $x_i$ из этой выборки соответствует $y_i$ из конечного множества. Рассмотрим множество $\{+1, -1\}$.
$$x_i\in \mathbb{R}^n,y_i=\{+1,-1\}, i=1,\ldots,m$$
То есть, есть два класса, метки у которых +1 и -1, и требуется построить функцию, которая разделяла бы $x_i$ на два класса.

Самый простой классификатор, который можно придумать, — это **линейный классификатор**, который выражается следующим образом:
$$\hat{y}=sign(w^Tx+b)$$
То есть, $y_i$ — это знак у линейной функции от $x$ по $w$.

То же самое можно немного усилить, то есть потребовать, чтобы для тех $x$, класс которых +1, значение функции должно быть больше единицы, а -1 — меньше -1:
$$\begin{cases}
w^Tx_i+b>1, & y_i=+1 \\
w^Tx_i+b<-1,& y_i=-1
\end{cases}$$
Это делается для того, чтобы совместить систему из неравенств в одно неравенство
$$y_i(w^Tx+b)>1$$
### пример

Например, есть два облака точек

![[optimization_01_05.jpg|w50]]


При этом оранжевые крестики — это класс -1, а голубые точки — класс +1.

Разделить классы можно, например, вот так

![[optimization_01_06.jpg|w50]]

По одну сторону от гиперплоскости одно облако точек, по другую — другое.
Провести гиперплоскоть можно разными способами:

![[optimization_01_07.jpg|w50]]

### возможные решения

У нас имеется два параметра $w$ — вектор нормали к гиперплоскости и $b$ — число, показывающее сдвиг относительно начала координат. Требуется поставить такую задачу оптимизации по отношению к этим двум параметрам, чтобы наши объекты были как можно лучше разделены получившейся гиперплоскостью.

Для определения гиперплоскости поможет концепция **максимизация зазора**. Зазор можно представить как минимальное расстояние между точками, принадлежащими разным выборкам. 

Опорные объекты — это те объекты, на которых достигается минимум расстояния. Требуется, чтобы для этих объектов были выполнены следующие два равенства:
$$\begin{cases}
w^Tx_k+b=1, & y_k=+1 \\
w^Tx_j+b=-1, & y_j=-1
\end{cases}$$
Важно отметить, что эти равенства всегда можно достигнуть, потому что $w$ — это вектор и $b$ — число, которые можно отмасштабировать таким образом, чтобы для тех $x_k$, которые определяются, как опорные, достигалось выполнение этой системы.

Поскольку каждое из уравнений задает некоторую гиперплоскость, можно посчитать между ними расстояние, которое вычисляется как модуль разности между правыми частями системы уравнений и отношение этой разности к норме $w$:
$$d=\frac{|c_1-c_2|}{||w|}=\frac{2}{||w||_2}$$
Поскольку нужно найти такие $w$ и $b$, чтобы это расстояние было как можно больше, то есть можно минимизировать обратное
$$\min_{w,b}\frac{1}{2}||w||_2^2$$
при том условии, что выполнено изначальное неравенство:
$$y_i(w^Tx+b)>1$$
которое гарантирует, что объекты из разных классов будут лежать по разные стороны от гиперплоскости.

Решением этой задачи будет следующий вариан оптимальной гиперплоскости:

![[optimization_01_08.jpg|w50]]

Результат был получен в предположении, что существует такая гиперплоскость, которая разделяет две выборки. Однако, бывает так, что выборки линейно неразделимы.

Тогда может существовать гиперповерхность, не гиперплоскость, которая разделяет выборки.