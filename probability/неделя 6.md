# Определение независимости двух событий и независимости в совокупности

Допустим, есть две случайных величины
$$\xi,\eta:\Omega\in\mathbb{R}$$
Пусть:
$$\begin{aligned}
&\xi:\Omega\to\{x_1,\dots,x_n\}\\
&\eta:\Omega\to\{y_1,\dots,y_m\}
\end{aligned}$$
При этом $\xi$ и $\eta$ **независимы**, если 
$$\forall i,j \ P(\xi=x_i,\eta=y_j)=P(\xi=x_i)P(\eta=y_j)$$

Можно рассматривать целые наборы случайных величин $\xi_1,\dots,\xi_n$ и говорить о независимости случайных величин внутри этих наборов. Можно говорить, что такие случайные величины попарно независимы, а также можно говорить о независимости в совокупности (взаимной независимости).

Случайные величины независимы в совокупности, если
$$\forall k \ \forall i_1,\dots,i_k \ \forall x_{i_1},\dots,x_{i_k}\ P(\xi_{i_1}=x_{i_1},\dots,\xi_{i_k}=x_{i_k})=P(\xi_{i_1}=x_{i_1})\times\ldots\times P(\xi_{i_k}=x_{i_k})$$

Пример ситуации, когда случайные величины $\xi_1,\dots,\xi_n$ попарно независимы, но зависимы в совокупности.

Возьмем события $A_1,\dots,A_n$, которые независимы попарно, но зависимы в совокупности. Для 
$$\xi_1=\mathnormal{I}_{A_1}(\omega)=\begin{cases}
1, \text{if} \ \omega\in A_1\\
0, \text{otherwise}
\end{cases}$$

# Математическое ожидание произведения независимых случайных величин

Пусть $\xi,\eta$ — независимы. Тогда
$$M(\xi\eta)=(M\xi)(M\eta)$$
Доказательство.

Пусть $\xi: \Omega \to \{x_1,\dots,x_n\}, \eta: \Omega \to \{y_1, \dots, y_m\}$.

Произведение двух случайных величин — это тоже случайная величина, которая на каждом элементарном событии $\omega$ принимает некоторое конкретное значение, равное $\xi(\omega)\eta(\omega)$.
$$M(\xi\eta)=\sum_{\omega\in\Omega}\xi(\omega)\eta(\omega)P(\omega)$$
Возьмем $\Omega_{i,j}\subset\Omega:\forall \omega\in\Omega_{i,j} \ \xi(\omega)=x_i,\eta(\omega)=y_j$.

Тогда $\Omega=\bigcup_{i=1}^n\bigcup_{j=1}^m\Omega_{i,j}$. При этом различные $\Omega_{i,j}$ (с разными индексами) не пересекаются.
$$M(\xi\eta)=\sum_{\omega\in\Omega}\xi(\omega)\eta(\omega)P(\omega)=\sum_{i=1}^n\sum_{j=1}^m\sum_{\omega\in\Omega_{i,j}}\xi(\omega)\eta(\omega)P(\omega)=$$
Если мы зафиксировали величины $i,j,\omega$, то $\xi(\omega)=x_i, \eta(\omega)=y_j$
$$=\sum_{i=1}^n\sum_{j=1}^m\sum_{\omega\in\Omega_{i,j}}x_iy_jP(\omega)=$$
Числа $x_i, y_j$ от $\omega\in\Omega_{i,j}$ никак не зависят, значит их можно вынести за знак суммирования
$$=\sum_{i=1}^n\sum_{j=1}^mx_iy_j\sum_{\omega\in\Omega_{i,j}}P(\omega)=\sum_{i=1}^n\sum_{j=1}^mx_iy_jP(\Omega_{i,j})=\sum_{i=1}^n\sum_{j=1}^mx_iy_jP(\xi=x_i,\eta=y_j)=$$
Так как $\xi$ и $\eta$ — независимые случайные величины, то вероятность будет равна произведению их вероятностей
$$=\sum_{i=1}^n\sum_{j=1}^mx_iy_jP(\xi=x_i)P(\eta=y_j)=$$
так как суммирования величин не зависят друг от друга
$$=\sum_{i=1}^nx_iP(\xi=x_i)\sum_{j=1}^my_jP(\eta=y_j)=$$
в соответствии со вторым определением математического ожидания
$$=(M\xi)(M\eta)$$

# Дисперсия суммы независимых случайных величин

Если умножить случайную величину на какую-то константу $c$, то дисперсия
$$D(c\xi)=c^2D\xi$$
Если посмотреть на дисперсию суммы случайных величин, то она не всегда будет равна сумме дисперсий
$$D(\xi_1+\ldots\xi_n)$$

Пусть $\xi_1,\ldots,\xi_n$ *независимы попарно*. Тогда дисперсия их суммы будет равна сумме дисперсий
$$D(\xi_1+\ldots+\xi_n)=D\xi_1 + \ldots+D\xi_n$$

**Доказательство**.

Если $\xi_1,\dots,\xi_n$ попарно независимы, то и величины $(\xi_1-M\xi_1),\dots,(\xi_n-M\xi_n)$ тоже попарно независимы. Обозначим эти разности как $\eta_1,\dots,\eta_n$.

По своему определению дисперсия суммы это
$$D(\xi_1+\ldots\xi_n)=M\big(\xi_1+\ldots+\xi_1-M(\xi_1+\ldots+\xi_n)\big)^2=$$
пользуемся линейностью математического ожидания
$$=M\big((\xi_1-M\xi_1)+(\xi_2-M\xi_2)+\ldots+(\xi_n-M\xi_n)\big)^2=$$
заменим для краткости
$$=M(\eta_1+\eta_2+\ldots+\eta_n)^2=M(\eta_1^2+ldots+\eta_n^2+\sum_{i\ne j}\eta_i\eta_j)=$$
снова применим линейность математического ожидания
$$=M\eta_1^2+\ldots+M\eta_n^2+\sum_{i\ne j}M(\eta_i\eta_j)=$$
случайные величины $\eta_i, \eta_j$ независимы, потому что они попарно независимы, поэтому математическое ожидание их произведения равно произведению их математических ожиданий
$$=M\eta_1^2+\ldots+M\eta_n^2+\sum_{i\ne j}M(\eta_i)M(\eta_j)=$$
проведем обратную замену
$$M(\eta_i)=M(\xi_i-M\xi_i)=0$$
выходит, что каждый множитель суммы равен нулю
$$=M\eta_1^2+\ldots+M\eta_n^2=M(\xi_1-M\xi_1)^2+\ldots+M(\xi_n-M\xi_n)^2=D\xi_1+\ldots+D\xi_n$$

# Существование двух зависимых некоррелированных случайных величин

Можно привести примеры таких случайных величин $\xi, \eta$, которые зависимы, но, тем не менее, $M(\xi\eta)=(M\xi)(M\eta)$.

Такие величичны называются **некоррелированными**.

# Закон больших чисел

Пусть $\xi_1,\dots,\xi_n,\dots$ — некоторая бесконечная последовательность попарно некоррелированных случайных величин. Пусть существует некоторая константа $c$, такая, что для любого $i$ дисперсия $\xi_i$ не превосходит $c$
$$\exists c:\forall i \ D\xi_i\le c$$
Тогда
$$\forall \varepsilon>0  \ P\bigg(\bigg|\frac{\xi_1+\ldots+\xi_n}{n}-\frac{M\xi_1+\ldots +M\xi_n}{n}\bigg|> \varepsilon\bigg)\xrightarrow[n\to\infty]{}0$$
В частности, если для любого $M\xi_i=a$, то
$$P\bigg(\bigg|\frac{\xi_1+\ldots+\xi_n}{n}-a\bigg|>\varepsilon\bigg)\xrightarrow[n\to\infty]{}0$$
**Доказательство.**

Если обозначить уменьшаемое в скобках как $\eta$, то вычитаемое будет $M\eta$ 
$$P\bigg(\bigg|\frac{\xi_1+\ldots+\xi_n}{n}-\frac{M\xi_1+\ldots +M\xi_n}{n}\bigg|> \varepsilon\bigg)$$

Тогда это будет в чистом виде неравенство Чебышева
$$P(|\eta-M\eta|>\varepsilon)\le\frac{D\eta}{\varepsilon^2}=$$

Так как случайные величины некоррелированы попарно, то дисперсия их суммы — это сумма дисперсий
$$\frac{1}{\varepsilon^2n^2}(D\xi_1+\ldots+D\xi_n)$$
каждая из дисперсий по условию ограничена константой $c$
$$\frac{1}{\varepsilon^2n^2}(D\xi_1+\ldots+D\xi_n)\le\frac{cn}{\varepsilon^2n^2}$$

правая часть неравенства стремится к нулю при $n\to 0$.

