	# условное математическое ожидание в дискретном случае

Пусть есть две случайные величины $\xi, \eta$ с дискретными распределениями, то есть вероятностное распределение каждого из них сконцентрировано не более, чем в счетном множестве.

$X_\xi, X_\eta$ — это не более, чем счетные множества значений, соответственно, случайных величин $\xi$ и $\eta$.

Можно взять какие-нибудь $x\in X_\xi, y \in X_\eta$ и вычислить вероятность
$$P(\xi=x|\eta=y)=\frac{P(\xi=x|\eta=y)}{P(\eta=y)}$$
Условное математическое ожидание
$$E(\xi|\eta=y)$$
то есть у нас есть не все распределение, а условное
$$E(\xi|\eta=y)=\sum_{x\in X_\xi}xP(\xi=x|\eta=y)$$
Функция справа — это функция от $y$, то есть $y$ зафиксирован и произведено суммирование по всем $x$. Для разных $y$ могут быть получены разные результаты. Если эту функцию принять за $g(y)$, то можно записать математическое ожидание
$$E(\xi|\eta)=g(\eta)$$

## пример вычисления

Есть игральная кость. Пусть $\xi$ — это выпавшее число, а $\eta$ — это наименьший делитель больше единицы для $\xi$:
$$\eta(1)=1,\eta(2)=2,\eta(3)=3,\eta(4)=2,\eta(5)=5,\eta(6)=2$$

Найдем математическое ожидание $E(\xi|\eta=1)$

$$E(\xi|\eta=1)=$$
в этом случае $\xi=1$, все остальные условные вероятности равны нулю
$$=P(\xi=1|\eta=1)=1$$
Теперь для $E(\xi|\eta=2)$
$$E(\xi|\eta=2)=2P(\xi=2|\eta=2)+4P(\xi=4|\eta=2)+6P(\xi=6|\eta=2)=4$$

для 3 и 5
$$E(\xi|\eta=3)=3$$
$$E(\xi|\eta=5)=5$$
Поэтому как функцию от $\eta$ можно записать следующим образом
$$E(\xi|\eta)=4(\eta=2)+\eta I(\eta\ne 2)$$

# определение условного математического ожидания в общем случае

Пусть есть две случайные величины $\xi, \eta$.

Условное математическое ожидание $\xi$ при условии $\eta$ — это случайная величина, которая является функцией от случайной величины $\eta$
$$E(\xi|\eta)=f(\eta)$$
где $f$ — борелевская функция такая, что выполнено следующее условие: какое бы ни было взято множество $B\in \mathcal{B}(\mathbb{R})$, должно быть справедливо равенство
$$Ef(\eta)I(\eta\in B)=E\xi I(\eta \in B)$$

# cледствие для дискретных случайных величин

Пусть $\eta$  — дискретная случайная величина. Тогда
$$\eta=\sum_iC_iID_i$$
сумма может быть как конечной, так и счетной. $D_i$ — это события, лежажие в $\sigma$-алгебре, на которых случайная величина принимает постоянное значение. Удобно считать, что у всех этих событий не нулевая вероятность.
$$P(D_i)>0$$
Математическое ожидание 
$$E(\xi|\eta)=\sum_i\frac{E\xi ID_i}{P(D_i)}ID_i$$
Если $\xi$ дискретна
$$E\xi ID_i=\sum xP(\xi=x|\eta=C_i)$$

Доказательство
Возьмем любое борелевское множество $B\in \mathcal{B}(\mathbb{R})$.

$$E\sum_i\frac{E\xi ID_i}{P(D_i)}ID_i I(\eta \in B)=$$
 $\eta$ — случайная величина, которая принимает не более, чем счетное число значений, поэтому в множестве $B$ нужно найти только те значения, которым может равняться $\eta$.
 $$=\sum_i\sum_{j:C_j\in B}E\frac{E\xi ID_i}{P(D_i)}I(\eta=C_j, \eta=C_i)=\sum_i\sum_{j:C_j\in B} \frac{E\xi ID_i}{P(D_i)}P(\eta = C_j,\eta=C_i)=$$
 Вероятность не равная нулю только тогда, когда $C_i=C_j$
 $$\sum_{i:C_i\in B} \frac{E\xi ID_i}{P(\eta=C_i)}P(\eta=C_i)=\sum_{i:C_i\in B}E\xi ID_i=E\xi I(\eta \in B)$$
 
# свойства условного математического ожидания

## линейность
$$E(c_1\xi_1 + c_2\xi_2|\eta)=c_1E(\xi_1|\eta)+c_2E(\xi_2|\eta)$$

## матожидание от условного матожидания
$$E\big(E(\xi|\eta)\big)=E\xi$$

## независимость
Если случайные величины $\xi$ и $\eta$ независимы
$$E(\xi|\eta)=E\xi$$

## матожидание функции при условии случайной величины
$$E\big[g(\eta)|\eta\big]=\xi$$
где $g$ — борелевская функция, $\xi=g(\eta)$.


## телескопическое свойство
$$E\big[E(\xi|g(\eta))|\eta\big]=E\big(\xi|g(\eta)\big)$$
$$E\big[E(\xi|\eta)|g(\eta)\big]=E\big(\xi|g(\eta)\big)$$

## вынос функции за матожидание
$$E\big[\xi g(\eta)|\eta\big]=g(\eta)E[\xi|\eta]$$

# вычисление условного математического ожидания

Возьмем гауссовский вектор, состоящий из двух случайных величин $(\xi,\eta)$. Допустим, известен вектор матожиданий $(1, -1)$ и матрица ковариаций
$$\begin{pmatrix}
1 & 1 \\
1 & 2
\end{pmatrix}$$

Найдем $E(\xi|\eta)$.

Рассмотрим вектор, линейно преобразованный $(\xi - c\cdot \eta, \eta)$. Необходимо подобрать такое $c$, чтобы $cov(\xi-c\eta,\eta)=0$
$$cov(\xi,\eta)-c \cdot cov(\eta,\eta)=1 - c\cdot 2=0$$
$$c=\frac{1}{2}$$
тогда вектор
$$(\xi-\frac{1}{2}\eta,\eta)$$

Раз $\xi-\frac{1}{2}\eta$ и $\eta$ независимы, то условное матожидание
$$E(\xi - \frac{1}{2}\eta|\eta)=E(\xi-\frac{1}{2}\eta)=\frac{3}{2}$$

Теперь воспользуемся свойством линейности
$$E(\xi|\eta)-\frac{1}{2}E(\eta|\eta)=\frac{3}{2}$$
это есть не что иное, как
$$E(\xi|\eta)-\frac{1}{2}\eta=\frac{3}{2}$$
$$E(\xi|\eta)=\frac{\eta+3}{2}$$

# подсчет условного матожидания в дискретном случае

Пусть есть последовательность из четырех случайных величины $\xi_1,\xi_2,\xi_3,\xi_4$, которые одинаково распределены и независимы. Они принимают два значения 1 и -1
$$P(\xi_1=1)=P(\xi_1=-1)=\frac{1}{2}$$
Получаем случайное блуждание.

Обозначим сумму первых $i$ случайных величин
$$S_i=\sum_{j=1}^i\xi_j$$
Найдем условное матожидание
$$E(\max_{i=1,\ldots,4} S_i|S_4=0)=$$
$$=0P(\max=0|S_4=0)+1P(\max=1|S_4=0)+ 2P(\max=2|S_4=0)=$$
$$= P(\max=1|S_4=0)+ 2P(\max=2|S_4=0)=$$
Есть $C_4^2$ способов, выйти из нуля и вернуться в ноль. Все эти способы равновероятны.
$$=\frac{3}{6}+\frac{2}{6}=\frac{5}{6}$$
если расписать по опредлению
$$P(\max=1|S_4=0)=\frac{P(\max=1,S_4=0)}{P(S_4=0)}=\frac{1/6}{6/16}=\frac{1}{6}$$


по формуле
$$E(\max S_i|S_4)=\sum_\alpha \frac{E\max S_iID_\alpha}{P(D_\alpha)}ID_\alpha$$
Здесь в качестве $D_\alpha$ выступают всевозможные события, которые могут стоять в условии, а именно в условии стоит то, чему может быть равно $S_4$. 

Если $S_4=-4$, то в этом случае максимум равен нулю
$$E(\max S_i|S_4=-4)=0$$
Если $S_4=-2$, то есть три ситуации, когда максимум равен нулю и одна, когда — единице
$$E(\max S_i|S_4=-2)=\frac{1}{4}$$
Если $S_4=2$
$$E(\max S_i| S_4=2)=\frac{3}{4}+\frac{2\cdot 3}{4}=\frac{9}{4}$$
Если $S_4=4$
$$E(\max S_i| S_4=2)=4$$

ответ
$$E(\max S_i| S_4)=\frac{1}{4}I(S_4=2)+\frac{5}{6}I(S_4=0)+\frac{9}{4}I(S_4=2)+4I(S_4=4)$$

# подсчет условного мат ожидания для двумерного гауссовского вектора

Рассмотрим гауссовский вектор, состоящий из двух компонент $(X,Y)$, задан вектор средних $(a_1, a_2)$ и матрица ковариаций
$$\begin{pmatrix}
\sigma_{11} & \sigma_{12} \\
\sigma_{21} & \sigma_{22}
\end{pmatrix}$$
Найдем $E(X^2|Y)$.

Найдем такое число $\alpha$, что $X-\alpha Y$ не зависит от $Y$, который является линейным преобразованием вектора $(X,Y)$
$$\begin{pmatrix}
X-\alpha Y \\
Y
\end{pmatrix}=
\begin{pmatrix}
1 & -\alpha \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
X \\
Y
\end{pmatrix}$$
Для независимости компонент гауссовского вектора необходимо и достаточно, чтобы их ковариация была равна нулю
$$cov(X-\alpha Y, Y)=0$$
по линейности
$$cov(X-\alpha Y, Y)=cov(X,Y)-\alpha cov(Y,Y)=\sigma_{12}- \alpha\sigma_{22}=0$$
выражение равно нулю, когда
$$\alpha=\frac{\sigma_{12}}{\sigma_{22}}$$
Считаем, что по условию вектор не вырожденный, а именно, дисперсия первой и второй компонент положительны. 

Значит величины независимы
$$\newcommand{\indep}{\perp \!\!\! \perp} X-\frac{\sigma_{12}}{\sigma_{22}}Y \indep Y$$
если левую часть возвести в квадрат, то она тоже не будет зависить от $Y$
$$\newcommand{\indep}{\perp \!\!\! \perp} \left( X-\frac{\sigma_{12}}{\sigma_{22}}Y \right)^2 \indep Y$$

Матожидание
$$E(X-\alpha Y)^2=E\big((X-\alpha Y)^2|Y\big)=$$
по линейности
$$=E(X^2|Y)-2\alpha E(XY|Y) + \alpha^2E(Y^2|Y)=$$
$$=E(X^2|Y)-2\alpha YE(X|Y)+\alpha^2Y^2$$

найдем матожидание $(X|Y)$
$$E(X-\alpha Y)=E(X-\alpha Y|Y)=E(X|Y)-\alpha Y$$
откуда
$$E(X|Y)=a_1-\alpha a_2+\alpha Y$$
найдем матожидание $(X-\alpha Y)^2$
$$E(X-\alpha Y)^2=EX^2-2\alpha EXY+\alpha^2EY^2=\sigma_{11}+a_1^2-2\alpha(\sigma_{12}+a_1a_2)+\alpha^2\sigma_{22}+\alpha^2a_2^2=$$
$$=\sigma_{11}-2\alpha\sigma_{12}+\alpha^2\sigma_{22}+(a_1-\alpha a_2)^2$$

теперь соберем все вместе
$$E(X^2|Y)=\sigma_{11}-2\alpha\sigma_{12}+\alpha^2\sigma_{22}+(a_1-\alpha a_2)^2+2\alpha Y(a_1-\alpha a_2+\alpha Y)-\alpha^2Y^2$$

# сумма в условии

Пусть есть $X,Y$ — независимые и одинаково распределенные случайные величины. Найдем $E(X|X+Y)$

Надо доказать, что $\forall B \in \mathcal{B}(\mathbb{R})$ выполнено $EXI(X+Y\in B)=E\frac{1}{2}\big(X+Y|I(X+Y\in B)\big)$

Заметим, что раз  $X,Y$ — независимые и одинаково распределенные случайные величины, то у векторов 
$$(X,Y )\overset{d}{=}(Y,X)$$

$$f_B(x,y)=xI(x+y\in B)$$
значит
$$f_B(X,Y)\overset{d}{=}f_B(Y,X)$$
тогда матожидания должны совпадать
$$Ef_B(X,Y)=Ef_B(Y,X)$$
получается
$$EXI(X+Y\in B)=EYI(X+Y\in B)$$
Раз совпадает, то каждое из них
$$EXI(X+Y\in B)= \frac{1}{2}E(X+Y)I(X+Y\in B)$$