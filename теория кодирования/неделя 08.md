# вероятностная модель канала связи

В вероятностной модели у канала один параметр $p$ — вероятность ошибки на символ. Когда в канал подается слово $\mathbf{a}=(a_1 \ldots a_n)$, на выходе из канала, если вероятность больше нуля, может получиться любое слово $\mathbf{a}'=(a_1' \ldots a_n')$.

В двоичном случае каждая инверсия бита может произойти с вероятностью $p$. При этом мы всегда может предполагать, что $p<\frac{1}{2}$. 

Если вероятность $p=\frac{1}{2}$, то канал является генератором случайного шума. 

Если $p>\frac{1}{2}$, то на выходе из канала можно поставить инвертор, который каждый бит в принятом из канала слове инвертирует и тогда канал превращается в канал, для которого вероятность искажения меньше $\frac{1}{2}$.

## скорость кодирования

Способы кодирования и декодирования — это функции, которые сопоставляют коротким словам более длинные слова и наоборот:
$$\begin{aligned}
E:\mathbb{F}_2^k\to\mathbb{F}_2^n\\
D:\mathbb{F}_2^n\to\mathbb{F}_2^k
\end{aligned}$$
Скорость кодирования — это отношение параметров $k$ и $n$. То есть, это то, во сколько раз увеличивается слово, при кодировании:
$$\text{rate}(E,D):=\frac{k}{n}$$
## энтропия

Функция энтропии:
$$H(p):=-p\log_2p-(1-p)\log_2(1-p)$$
график функции энтропии выглядит следующим образом:
![[неделя 08.png]]
Максимальная точка — это $\frac{1}{2}$, максимальное значение — 1.

## лемма о числе ошибок

Пусть $p$ и $\gamma$ таковы, что $p\in\big(0,\frac{1}{2}\big)$ и $\gamma > 0$.

Пусть $\mathbf{e}\in\mathbb{F}^n_2$ — случайный вектор, каждая компонента которого равна единице с вероятностью $p$.

Тогда:
$$Pr\big[\|\mathbf{e}\|> n(p+\gamma)\big]\le c^n$$
где $c<1$ и $c$ зависит только от $p$ и $\gamma$.

**доказательство**

Пусть произвольное $t> 0$.
Пусть $e_1,\ldots,e_n$ — компоненты вектора $\mathbf{e}$.

Перепишем неравенство, взяв экспоненту и домножив обе части на $t$:
$$Pr\big[\|\mathbf{e}\|> n(p+\gamma)\big] = Pr\big[\exp(t\cdot\|\mathbf{e}\|)> \exp\big(t\cdot n(p+\gamma)\big)\big]=$$
представим вес вектора $\mathbf{e}$ как сумму координат вектора:
$$=Pr\big[\exp\big(t\cdot\sum e_i\big)>\exp\big(t\cdot n(p+\gamma)\big)\big]$$
воспользуемся неравенством Маркова:
$$=Pr\big[\exp\big(t\cdot\sum e_i\big)>\exp\big(t\cdot n(p+\gamma)\big)\big]\le\frac{\mathbb{E}[\exp(t\cdot\sum e_i)]}{\exp(t\cdot n(p+\gamma))}=$$
перенесем знаменатель, добавив знак минус, а экспоненту от суммы превращаем в произведение экспонент:
$$=\exp\big(-t\cdot n(p+\gamma)\big)\cdot\mathbb{E}\bigg[\prod\exp(te_i)\bigg]=$$
так как случайные величины независимы:
$$=\exp\big(-t\cdot n(p+\gamma)\big)\cdot \prod\mathbb{E}\big[\exp(t e_i)\big]=$$
теперь вынесем $n$ в степень:
$$=\big(\exp(-t(p+\gamma)\big)\cdot\mathbb{E}\big[\exp(te_i)\big]\big)^n=$$
посчитаем матожидание по определению:
$$=\big(\exp(-t(p+\gamma)\big)\cdot (1-p+p\exp(t))\big)^n$$
Хотим доказать, что при малых $t$ выполнено:
$$\big(\exp(-t(p+\gamma)\big)\cdot (1-p+p\exp(t))\big)^n< 1$$
перепишем:
$$1-p+p\exp(t)<\exp\big(t(p+\gamma)\big)$$
cначала посмотрим на левую часть неравенства и оценим по Тейлору:
$$1-p+p\exp(t)=1-p+p\big(1+t+O(t^2)\big)=1+pt+O(t^2)$$
теперь с правой частью по Тейлору:
$$\exp\big(t(p+\gamma)\big)=1+(p+\gamma)t+O(t^2)$$
получается, что при всех достаточно маленьких $t$ неравенство выполнено
$$1-p+p\exp(t)<\exp\big(t(p+\gamma)\big)$$

## теорема Шеннона (о существовании кода)

Пусть есть сколь угодно маленькое число $\varepsilon > 0$ и вероятность ошибки на один символ в канале $p\in \big(0, \frac{1}{2}\big)$.

Тогда найдутся такие $n$ и $k$, и такие $E$ и $D$, что:
$$\text{rate}(E,D)\ge 1-H(p)-\varepsilon$$
и при случайном выборе слова для кодирования $\mathbf{a}\in\mathbb{F}_2^k$, то при подаче закодированного блока в канал и при возникновении в канале случайных ошибок:
$$Pr\big[D(E(\mathbf{a})+\mathbf{e})\ne\mathbf{a}\big]<\varepsilon$$
Это вероятность неправильно декодировать слово.

### доказательство
#### случай 1

Зафиксируем $\varepsilon> 0$.
Возьмем любое $\gamma$, удовлетворяющее условиям:
$$\begin{aligned}
p+\gamma &< \frac{1}{2}\\
H(p+\gamma) &< H(p)+\frac{\varepsilon}{2}
\end{aligned}$$
Пусть $n$ и $k$ достаточно велики и при этом:
$$\frac{k}{n}=1-H(p)-\varepsilon$$
Возьмем случайную функцию:
$$E:\mathbb{F}_2^k\to\mathbb{F}^n_2$$
Для заданной функции $E$ определим функцию $D$ так:
- $D(\mathbf{b}) = \mathbf{a}$, если $\mathbf{a}$ — единственное слово, для которого
$$d\big(E(\mathbf{a}),\mathbf{b}\big)\le n(p+\gamma)$$
- $D(\mathbf{b})=0$, если такое $\mathbf{a}$ не существует или не единственное.

Временно зафиксируем $\mathbf{a}\in \mathbb{F}_2^k$, и оценим
$$Pr_\mathbf{a}\big[D\big(E(\mathbf{a})+\mathbf{e}\big)\ne \mathbf{a}\big]$$
Декодирование может пройти неверно, если:
- Ошибок слишком много
$$d\big(E(\mathbf{a})+\mathbf{e},E(\mathbf{a})\big) > n(p+\gamma)$$
По лемме о числе ошибок вероятность этого не больше $c^n$, где $c<1$.

- Ошибок немного, но схема кодирования выбрана неудачно:
$$d\big(E(\mathbf{a})+\mathbf{e},E(\mathbf{a})\big) \le n(p+\gamma)$$
$$\exists\mathbf{a}'\ne\mathbf{a}, d\big(E(\mathbf{a})+\mathbf{e},E(\mathbf{a'})\big) \le n(p+\gamma)$$

#### случай 2

Для каждого фиксированного $\mathbf{a}'\in\mathbb{F}_2^n$ по формуле полной вероятности имеем:
$$\begin{aligned}
&Pr_{\mathbf{a}}\big[d\big(E(\mathbf{a})+\mathbf{e},E(\mathbf{a})'\big)\le n(p+\gamma)\big] = \\
&=\sum_{\mathbf{x,y}}Pr_{\mathbf{a}}\big[d\big(E(\mathbf{a})+\mathbf{e},E(\mathbf{a}')\big)\le n(p+\gamma)|E(\mathbf{a})=\mathbf{x}, \mathbf{e}=\mathbf{y}\big]\cdot Pr_\mathbf{a}\big[E(\mathbf{a})=\mathbf{x},\mathbf{e}=\mathbf{y}\big]
\end{aligned}$$
Если зафиксировать вектор $\mathbf{e}$ и то, куда переходит вектор $\mathbf{a}$, то у нас полностью определен вектор $E(\mathbf{a})+\mathbf{e}$.

Слово $\mathbf{a}'$ фиксировано, при этом $E(\mathbf{a}')$ — случайный вектор длины $n$.

Вероятность того, что слову $\mathbf{a}'$ сопоставляется конкретный вектор из нулей и единиц равна $\big(\frac{1}{2})^n$.

Выбрать вектор, отклоняющийся от другого фиксированного вектора не более, чем в $n(p+\gamma)$ компонентах, как сумма биномиальных коэффициентов:
$$Pr_{\mathbf{a}}\big[d\big(E(\mathbf{a})+\mathbf{e},E(\mathbf{a}')\big)\le n(p+\gamma)|E(\mathbf{a})=\mathbf{x}, \mathbf{e}=\mathbf{y}\big]=2^{-n}\cdot\sum_{k=0}^{(p+\gamma)n}\binom{n}{k}$$
У нас есть энтропийная оценка:
$$2^{-n}\cdot\sum_{k=0}^{(p+\gamma)n}\binom{n}{k}\le2^{n\cdot \big(H(p+\gamma)-1\big)}$$
-1 в степени появляется, потому что в левой части неравенства $2^{-n}$. Так как у нас есть условие на $\gamma$, то можно оценить сверху:
$$2^{-n}\cdot\sum_{k=0}^{(p+\gamma)n}\binom{n}{k}\le2^{n\cdot \big(H(p+\gamma)-1\big)}<2^{n\cdot \bigg(H(p)+\frac{\varepsilon}{2}-1\bigg)}$$
Отсюда оценка:
$$\begin{aligned}
Pr_{\mathbf{a}}\big[d\big(E(\mathbf{a}),E(\mathbf{a}')\big)\le n(p+\gamma)\big] &< 2^k \cdot 2^{n\cdot \big(H(p)+\frac{\varepsilon}{2}-1\big)}=2^{n\cdot \bigg(\frac{k}{n}+H(p)+\frac{\varepsilon}{2}-1\bigg)}=\\
&=2^{n\cdot \bigg(1-H(p)-\varepsilon+H(p)+\frac{\varepsilon}{2}-1\bigg)} = \\
&= 2^{-\frac{\varepsilon}{2}n}
\end{aligned}$$
При фиксированном $\mathbf{a}$ и случайно выбираемых $E$ и $\mathbf{e}$ имеем
$$Pr_{\mathbf{a}}\big[D\big(E(\mathbf{a})+\mathbf{e}\big)\ne \mathbf{a}\big]< c^n+ 2^{-\frac{\varepsilon}{2}\cdot n} < \hat{c}^n$$
для некоторой константы $\hat{c} < 1$.

При случайном выборе $\mathbf{a}$, $\mathbf{e}$ и $E$ имеем:
$$\begin{aligned}
Pr\big[D\big(E(\mathbf{a})+\mathbf{e}\big)\ne \mathbf{a} \big] &=\sum_{x\in\mathbb{F}_2^k}2^{-k}\cdot Pr_{\mathbf{a}}\big[D\big(E(\mathbf{a})+\mathbf{e}\big)\ne \mathbf{a}|\mathbf{a}=\mathbf{x}\big] \\ 
<\sum_{x\in\mathbb{F}_2^k}2^{-k}\cdot \hat{c}^n=\hat{c}^n
\end{aligned}$$
При случайном выборе $\mathbf{a}$, $\mathbf{e}$ и $E$ имеем:
$$Pr\big[D(E(\mathbf{a})+\mathbf{e})\ne\mathbf{a}\big]< \hat{c}^n$$
для некоторой константы $\hat{c} < 1$.

Значит существует такое кодирование $E$, что при случайном выборе $\mathbf{a}$ и $\mathbf{e}$ выполнено:
$$Pr\big[D(E(\mathbf{a})+\mathbf{e})\ne\mathbf{a}\big]< \hat{c}^n$$
## теорема Шеннона (о пределе скорости)

Пусть $\varepsilon>0$ и $p\in\big(0, \frac{1}{2})$.

Тогда для всех достаточно больших $n$ и $k$ и любых $E$ и $D$, таких, что
$$\text{rate}(E,D)\ge1- H(p)+\varepsilon$$
при случайном выборе слова $\mathbf{a}\in \mathbb{F}_2^k$
$$Pr\big[D(E(\mathbf{a})+\mathbf{e})=\mathbf{a}\big]< \varepsilon$$
То есть, если скорость превышает на сколь угодно малое значение величину $1-H(p)$, то будет очень много ошибок.


## каскадные коды

Пусть:
- $C_{\text{int}}$ — $(n,m,d)_q$-код (внутренний код)
- $C_{\text{ext}}$ — $(N,M,D)_m$-код (внешний код)

Между этими кодами должны быть выполнено условие согласования, которое состоит в том, что кодовый алфавит, в котором рассматривается внешний код по мощности должен совпадать с количеством слов $m$ внутреннего кода.

Берем множество кодовых слов $C_{\text{int}}$, мощностью $m$. Каждое слово этого кода — набор отдельных символов. 

Есть внешний код $C_{\text{ext}}$, в котором каждое слово — набор отдельных символов $s_1 \ldots s_m$, 
которые извлекаются из некоторого алфавита $A_m$ мощности $m$.

Сопоставим каждому символу алфавита $A_m$ одно кодовое слово из кода $C_{\text{int}}$.

Берем каждое слово $C_{\text{ext}}$ и каждый символ алфавита $A_m$ заменяем на последовательность, которую представляет собой кодовое слово в $C_{\text{int}}$.

Число слов осталось тем же, что было в коде $C_{\text{ext}}$ — $M$.

Каждый символ, которых было $N$, заменился на одно слово из $C_{\text{int}}$, длина которого $n$, то общее количество символов $Nn$.

В $C_{\text{ext}}$ кодовое расстояние становится $Dd$.

Получаем каскадный код $(Nn, M, d')_q$-код, где $d'\ge Dd$

### каскадные линейные двоичные коды

Пусть:
- $C_{\text{int}}$ — $[n,k,d]$-код (внутренний код)
- $C_{\text{ext}}$ — $[N,K,D]_{2^k}$-код (внешний код)

Получаем двоичный код, потому что берем кодовое слово внешнего кода, у которого каждый элемент — это элемент поля $\mathbb{F}_2^k$ и вместо элемента поля подставляем целое слово внутреннего кода. Это слово имеет длину $n$.

Длина кодового слова получается $Nn$.

Линейный код получается не во всех случаях. Но есть возможность подобрать такое соответствие, что получится линейный код.

Линейной комбинации элементов поля соответствует линейная комбинация двоичных векторов.

Линейная комбинация двух векторов в итоговом каскадном коде представляет собой вектор, у которого каждая компонента — это линейная комбинация поля $\mathbb{F}_2^k$, которая является линейной комбинацией слов внутреннего кода. Линейная комбинация слов внутреннего кода — это слово внутреннего кода. 

Получаем каскадный $[Nn, kK, d']$-код, где $d'\ge dD$.

## асимптотически хорошие коды

Асимптотическая скорость:
$$\text{rate}(\tilde{C}):=\lim_{n\to\infty}\frac{\log_2|C_n|}{n}$$
Асимптотическое относительное кодовое расстояние:
$$\delta(\tilde{C}):=\lim_{n\to\infty}\frac{d(C_n)}{n}$$
Семейство кодов асимптотически хорошее, если для него
$$\begin{aligned}
\text{rate}(\tilde{C}) &> 0\\
\delta(\tilde{C}) &> 0
\end{aligned}$$
Из доказательства теоремы Шеннона следует, что для любого $\varepsilon$ существуют семейства
$$\text{rate}(\tilde{C})\ge1 - H\big(\delta(\tilde{C})\big)-\varepsilon$$

Семейство двоичных кодов:
$$\tilde{C}:=\{C_n\}^\infty_{n=1}$$
## следствие теоремы Варшамова-Гилберта

Если $\delta<0.5$ и $\rho$ таковы, что $H(\delta)\le 1-\rho$, то существует семейство линейных кодов $\tilde{C}$, для которого $\text{rate}(\tilde{C})\ge \rho$ и $\delta(\tilde{C}) \ge \delta$.

## каскадный код Форни

Пусть $t\in \mathbb{N}$ и пусть $\delta< 0.5$.

Внутренний код берем Варшамова-Гилберта:
$$\bigg[\frac{t}{1-H(\delta)},t,\frac{\delta t}{1-H(\delta)}\bigg]_{2^t}$$
Внешний код берем Рида-Соломона:
$$[2^t,2^{t-1},2^{t-1}+1]_{2^t}$$
Получаем каскадный $[n,k,d]$-код, для которого
$$\begin{aligned}
n &= \frac{t\cdot 2^t}{1-H(\delta)} \\
k &= \frac{t \cdot 2^t}{2} \\
d &> \frac{\delta t\cdot 2^t}{2\big(1-H(\delta)\big)}
\end{aligned}$$
Существует полиномиальный алгоритм, декодирующий кодовые слова, принятые с не более чем $\frac{\delta t\cdot 2^t}{8\big(1-H(\delta)\big)}$ ошибками.

Слово в каскадном коде представляет собой слово, состоящее из $2^t$ символов. На выходе принимаем возможно искаженное двоичное слово:
$$\mathbf{c}=\mathbf{a}_1\mathbf{a}_2\ldots\mathbf{a}_{2^t}\to\mathbf{\tilde{c}}=\mathbf{\tilde{a}}_1\mathbf{\tilde{a}}_2\ldots\mathbf{\tilde{a}}_{2^t}$$
Декодируем код в два этапа: разобьем слово на части, каждая из которых имеет длину слов внутреннего кода $\frac{t}{1-H(\delta)}$.

Декодируем каждое $\tilde{\mathbf{a}}$ перебором, получив в лучшем случае $\mathbf{a}$. Для того, чтобы слово было декодировано неправильно, нужно, чтобы последовательность битов исказилась очень сильно и произошло не менее, чем половина кодового расстояния Варшамова-Гилберта ошибок 
$$\frac{2\delta t}{1-H(\delta)}$$
Количество слов внутреннего кода, которые были неверно декодированы, оценивается сверху отношением величин $\frac{\delta t\cdot 2^t}{8\big(1-H(\delta)\big)}$ и $\frac{2\delta t}{1-H(\delta)}$, которое равно $\frac{2^t}{4}=2^{t-2}$.

Каждой последовательности битов соответствует символ поля, относительно которого строится внешний код.

Теперь переходим к внешнему коду. Код Рида-Соломона имеет кодовое расстояние $2^t-1$, и эта величина вдвое больше, чем $2^{t-2}$.

Получается, что мы можем восстановить исходное слово $\mathbf{c}$.