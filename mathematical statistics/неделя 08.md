# линейная регрессионная модель

С помощью линейной регрессионной модели можно сделать предсказания, относительно будущего поведения искомой величины в модели. 

Допустим, есть наблюдения и признаки этого наблюдения. Решается задача нахождения следующего наблюдения, при условии что его признаки будут фиксированными.

Пусть $X$ — случайное наблюдение из $\mathbb{R}^n$, причем, $X$ представимо в следующем виде:
$$X=l+\varepsilon$$
где $l$ — неслучайный вектор из $\mathbb{R}^n$, $\varepsilon$ — случайный вектор такой, что **его матожидание равно нулю** и матрица ковариаций представима в следующем виде:
$$\text{Var }\varepsilon=\sigma^2\cdot E_n$$
где $E_n$ (или $I_n$) — это единичная матрица размера $n\times n$.

Основное предположение линейной регрессии состоит в том, что $l$ можно просто описать с помощью $k$ признаков.

Пусть $l\in L$, где размерность подпространства $L$ равно $k$ и при этом $k < n$. Для реальной задачи лучше, если $k$ много меньше $n$. Задача состоит в том, чтобы по наблюдению $X$ получить оценку на $l$.

Пусть базис пространства $L$ — это $(Z_1, \ldots, Z_k)$. Таким образом, сам вектор $l$ выражается с помощью линейной комбинации этих векторов (разложение по базису)
$$l = Z_1\theta_1+\ldots+Z_k\theta_k=Z\theta$$
где $Z$ — матрица базиса размера $n\times k$, $\theta$ — вектор в пространстве $\mathbb{R}^k$.

## оценка методом наименьших квадратов (ОНК)

Оценка $\widehat\theta$ будет получаться как точка $\theta$, где будет достигаться минимум следующего выражения
$$\widehat\theta=\arg_\theta\min\|Y-Z\theta\|^2$$

То есть, нужно найти минимальное расстояние от $Y$ до подпространства $L$

![[Pasted image 20230701143206.png|w50]]

где $Z\widehat\theta$ — проекция вектора $Y$ на $L$
$$Z\widehat\theta=\text{proj}_L Y$$
$\widehat\theta$ — называется оценкой методом наименьших квадратов (о.н.к.).
$$\widehat\theta=(Z^TZ)^{-1}Z^TY$$
$Z$ нельзя перенести в левую часть уравнения, потому что это не квадратная матрица.

Продифференцируем выражение $\|Y-Z\theta\|^2$ и найдем, когда достигается его минимум. 
$$\begin{aligned}
\|Y-Z\theta\|^2&=(Y-Z\theta)^T(Y-Z\theta)=\\
&=Y^TY-Y^TZ\theta-\theta^TZ^TY+\theta^TZ^TZ\theta
\end{aligned}$$
В результате взятия нормы получается число, поэтому при раскрытии скобок все элементы будут числами, например 
$$Y^TZ\theta$$
здесь $Y$ — вектор размерности $n$, $Z$ — матрица размера $n\times k$ и $\theta$ — вектор размерности $k$. Соответственно $Z\theta$ — вектор размерности $n$, и при умножении на $Y$ — будет число.

То есть, выражение $\theta^TZ^TY$ можно транспонировать и получить $Y^TZ\theta$.
$$\begin{aligned}
\|Y-Z\theta\|^2&=(Y-Z\theta)^T(Y-Z\theta)=\\
&=Y^TY-Y^TZ\theta-\theta^TZ^TY+\theta^TZ^TZ\theta = \\
&=Y^TY-2Y^TZ\theta+\theta^TZ^TZ\theta
\end{aligned}$$
теперь можно продифференцировать
$$\begin{aligned}
\big(\|Y-Z\theta\|^2\big)'_{\theta_i}&=0-2(Y^TZ)_i+2(\theta^TZ^TZ)_i=0
\end{aligned}$$
$$-Y^TZ+\theta^TZ^TZ=0$$
$$\theta^TZ^TZ=Y^TZ$$
$$Z^TZ\theta=Z^TY$$
Так как $Z^TZ$  — квадратная матрица составленная из базисных векторов (и ее ранг равен $k$), то можно взять ее обратную матрицу (которая очевидно существует)
$$\widehat\theta=(Z^TZ)^{-1}Z^TY$$
### свойства ОНК

Оценка методом наименьших квадратов является несмещенной и наилучшей в среднеквадратическом подходе в классе оценок, которые линейны относительно вектора $Y$. 

#### матожидание и матрица ковариаций

Найти $E\widehat\theta$ и $\text{Var }\widehat\theta$.
$$\widehat\theta=(Z^TZ)^{-1}Z^TY$$
$$\begin{aligned}
E\widehat\theta=(Z^TZ)^{-1}Z^TEY=
\end{aligned}$$
так как $Y=Z\theta + \varepsilon$, а $E\varepsilon =0$, то матожидание $EY=Z\theta$
$$=(Z^TZ)^{-1}Z^TZ\theta=\theta$$
Теперь найдем матрицу ковариаций
$$\text{Var }\theta=\text{Var }(Z^TZ)^{-1}Z^TY=$$
вынесем матрицу $(Z^TZ)^{-1}Z^T$ с двух сторон из матрицы ковариаций
$$=(Z^TZ)^{-1}Z^T \text{Var }Y Z(Z^TZ)^{-1}=$$
$Z\theta$ не влияет на ковариационную матрица вектора $Y$, поэтому остается ковариационная матрица вектора $\varepsilon$, которая равна $\sigma^2\cdot I_n$ 
$$\begin{aligned}
&=(Z^TZ)^{-1}Z^T\sigma^2I_nZ(Z^TZ)^{-1}= \\
&=\sigma^2(Z^TZ)^{-1}(Z^TZ)(Z^TZ)^{-1} = \\
&=\sigma^2(Z^TZ)^{-1}
\end{aligned}$$
Несмещенной оценкой для параметра $\sigma^2$ будет оценка
$$\sigma^2=E\frac{1}{n-k}\|Y-Z\widehat\theta\|^2$$
### пример поиска ОНК

$X_1, X_2, X_3$ — взвешивания объектов с весами $a, b$ и $a+b$.  Причем взвешивание производится на одних и тех же весах и ошибка измерения всегда одна и та же (случайная величина с одним и тем же распределением). Требуется получить оценку методом наименьших квадратов для $a$ и $b$.

Нужно представить задачу в виде линейной регрессионной модели
$$X=\begin{pmatrix}
X_1\\
X_2\\
X_3
\end{pmatrix}=
\begin{pmatrix}
1 & 0 \\
0 & 1 \\
1 & 1
\end{pmatrix}
\begin{pmatrix}
a\\
b
\end{pmatrix}+\varepsilon$$
$$Z^TZ=\begin{pmatrix}
1 & 0 & 1\\
0 & 1 & 1
\end{pmatrix}\begin{pmatrix}
1 & 0 \\
0 & 1 \\
1 & 1
\end{pmatrix}=
\begin{pmatrix}
2 & 1 \\
1 & 2
\end{pmatrix}$$
$$(Z^TZ)^{-1}=\frac{1}{3}\begin{pmatrix}
2 & -1\\
-1 & 2
\end{pmatrix}$$
$$\begin{aligned}
\begin{pmatrix}
\widehat a\\
\widehat b
\end{pmatrix} &=\frac{1}{3}\begin{pmatrix}
2 & -1 \\
-1 & 2
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 1\\
0 & 1 & 1
\end{pmatrix}X=\\
&=\frac{1}{3}
\begin{pmatrix}
2 & -1 & 1\\
-1 & 2 & 1
\end{pmatrix}X=\\
&=\begin{pmatrix}
\frac{2}{3}X_1-\frac{1}{3}X_2+\frac{1}{3}X_3\\
-\frac{1}{3}X_1+\frac{2}{3}X_2+\frac{1}{3}X_3
\end{pmatrix}
\end{aligned}$$
## оптимальность ОНК в гауссовской линейной модели
$$\varepsilon \sim\mathcal{N}(0, \sigma^2I_n)$$
в данной модели оценки методом наименьших квадратов являются оптимальными оценками параметров $\theta$ и $\sigma^2$.

Вектор проекция $X$ на $L$ и норма проекции $X$ на $L$ ортогональное дополнение является полной достаточной статистикой в данной модели
$$\big(\text{proj}_LX,\|\text{proj}_{L\perp}X\|^2\big)$$

Для доказательства достаточно расписать плотность $X$
$$X=l+\varepsilon\sim\mathcal{N}(l, \sigma^2I_n)$$
можно записать правдоподобие наблюдения $X$
$$f_\theta(X)=\bigg(\frac{1}{\sqrt{2\pi\sigma^2}}\bigg)^n\exp\bigg(-\frac{1}{2\sigma^2}\sum_{i=1}^n(X_i-l_i)^2\bigg)=$$Можно заметить, что $\sum_{i=1}^n(X_i-l_i)^2$ — норма в евклидовой метрике, поэтому можно записать
$$=\bigg(\frac{1}{\sqrt{2\pi\sigma^2}}\bigg)^n\exp\bigg(-\frac{1}{2\sigma^2}\|X-l\|\bigg)=$$
по теореме Пифагора, если взять проекцию на $L$, а потом на ортогональное дополнение, то их норма в квадрате будет равна норме проецируемого вектора
$$=\bigg(\frac{1}{\sqrt{2\pi\sigma^2}}\bigg)^n\exp\bigg(-\frac{1}{2\sigma^2}(\|\text{proj}_LX-l\|^2+\|\text{proj}_{L\perp}X\|^2\bigg)$$
$l$ имеет $k$ параметров, $\sigma$ — 1 параметр, проекция $X$ на $L$ описывается $k$ параметрами, проекция $X$ на ортогональное дополнение — просто число (одномерная статистика). Таким образом, из теоремы об экспоненциальном семействе можно заключить, что $\big(\text{proj}_LX,\|\text{proj}_{L\perp}X\|^2\big)$ — полная достаточная статистика.

$\widehat\theta$  — это оптимальная оценка вектора параметров $\theta$, $\widehat\sigma^2$ — оптимальная оценка $\sigma^2$
$$\widehat\theta=(Z^TZ)^{-1}Z^TX$$
$$\text{proj}_LX=Z\widehat\theta=Z(Z^TZ)^{-1}Z^TX$$
$$(Z^TZ)^{-1}Z^T(Z\widehat\theta)=\widehat\theta$$
Получается, что $\widehat\theta$ является функцией от полной достаточной статистики.

$\widehat\theta$ — несмещенная оценка параметра $\theta$.

Это означает, что $\widehat\theta$ является оптимальной оценкой параметра $\theta$ по теореме об оптимальной оценке.
$$\sigma^2=\frac{1}{n-k}\|X-Z\widehat\theta\|^2=\frac{1}{n-k}\|\text{proj}_{L\perp}X\|^2$$
$\widehat\sigma^2$ — несмещенная оценка для параметра $\sigma^2$. Отсюда получаем, что $\widehat\sigma^2$ —  оптимальная оценка для параметра $\sigma^2$.

# теорема об ортогональном разложении гауссовского вектора

Пусть $X$ имеет многомерное нормальное распределение $\mathcal{N}(l, \sigma^2I_n)$, кроме того, пусть пространство $\mathbb{R}^n$ разложено в прямую сумму ортогональных подпространств
$$\mathbb{R}^n=L_1\oplus\ldots\oplus L_m$$
Обозначим $Y_j$ проекцию $X$ на соответствующее подпространство $L_j$
$$Y_j=\text{proj}_{L_j}X$$
Случайные величины $Y_j$ независимы в совокупности. Математическое ожидание $Y_j$ — это проекция $l$ на $L_j$
$$EY_j=\text{proj}_{L_j}l$$
тогда
$$\frac{1}{\sigma^2}\|Y_j-\text{proj}_{L_j}l\|^2\sim\chi^2_{\dim L_j}$$

# распределение Стьюдента

Пусть есть $\xi_0, \xi_1,\ldots,\xi_n$ — независимое стандартное нормальное распределение $\mathcal{N}(0,1)$ случайной величины. Тогда величина 
$$\eta=\frac{\xi_0}{\sqrt{\sum_{i=1}^n\xi_i^2/n}}\sim St(n)$$
имеет распределение Стьюдента с $n$ степенями свободы.

Величина в знаменателе под корнем имеет распределение $\chi^2$ с $n$ степенями свободы.

**свойства**:
1. распределение Стьюдента симметрично относительно нуля
2. при больших $n$ распределение Стьюдента похоже на стандартное нормальное распределение $\mathcal{N}(0,1)$


# доверительное оценивание параметров гауссовской линейной модели

$$\widehat\theta\sim\mathcal{N}\big(\theta,\sigma^2(Z^TZ)^{-1}\big)$$
$\widehat\theta$ — гауссовский вектор, потому что он является линейным преобразованием вектора $X$, который является гауссовским.

Допустим, требуется построить доверительный интервал для параметра $\theta_i$, который является компонентой вектора $\theta$.
$$a_{ii}=(Z^TZ)^{-1}$$
Если взять $i$ координату вектора $\theta_i$, то она будет иметь нормальное распределение с параметрами $\theta_i$ и $\sigma^2\cdot a_{ii}$
$$\widehat\theta_i\sim\mathcal{N}(\theta,\sigma^2\cdot a_{ii})$$
уберем все параметры
$$\frac{\widehat\theta_i-\theta_i}{\sqrt{\sigma^2\cdot a_{ii}}}\sim\mathcal{N}(0,1)$$
Проблема в том, что $\sigma^2$ неизвестна. Вместо нее нужно подставить оценку. Из теоремы об ортогональном разложении гауссовского вектора следует
$$\frac{1}{\sigma^2}\|Y-Z\widehat\theta\|^2\sim\chi^2_{n-k}$$
оценка $\sigma^2$ была
$$\sigma^2=\frac{1}{n-k}\|X-Z\widehat\theta\|^2$$
тогда
$$\frac{\widehat\theta_i-\theta_i}{\sqrt{\sigma^2\cdot a_{ii}}}\cdot\frac{1}{\sqrt{\widehat\sigma^2/\sigma^2}}$$
во-первых, $\sigma^2$ сократится; во-вторых, мы поделили нормальную случайную величину на случайную величину, которая имеет распределение $\chi^2$ деленную на $(n-k)$. То есть, должно получиться распределение Стьюдента. 
$$\frac{\widehat\theta_i-\theta_i}{\sqrt{\sigma^2\cdot a_{ii}}}\cdot\frac{1}{\sqrt{\widehat\sigma^2/\sigma^2}}\sim St(n-k)$$
если сократить
$$\frac{\widehat\theta_i-\theta_i}{\sqrt{a_{ii}-\widehat\sigma^2}}\sim St(n-k)$$
Таким образом, заключая эту величину между двумя квантилями распределения Стьюдента мы получаем доверительный интервал нужного уровня доверия.

Допустим, требуется доверительный интервал уровня доверия $1-\alpha$. Тогда 
$$P\bigg(t_{\alpha/2}<\frac{\widehat\theta_i-\theta_i}{\sqrt{a_{ii}-\widehat\sigma^2}}<t_{1-\alpha/2}\bigg)=1-\alpha$$
Доверительный интервал получается точный.

Так как квантили распределения Стьюдента симметричны относительно нуля, то доверительный интервал для $\theta_i$:
$$\bigg(\widehat\theta_i-t_{1-\alpha/2}\sqrt{a_{ii}\cdot\widehat\sigma^2}, \widehat\theta_i+t_{1-\alpha/2}\sqrt{a_{ii}\cdot\widehat\sigma^2}\bigg)$$
