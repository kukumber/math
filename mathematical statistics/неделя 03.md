# метод моментов

В основе метода моментов лежит следующее наблюдение. Например, требуется оценить параметры некоторого распределения, пусть это будет нормальное распределение. 

Параметрами нормального распределения являются $a$ и $\sigma^2$. Если взять математическое ожидание от членов выборки то получится $a$, если взять математическое ожидание от квадратов членов выборки, то получится $(\sigma^2+a^2)$. 

Эти два момента полностью определяют вектор параметров $(a, \sigma^2)$. В основе этого наблюдения и лежит метод моментов.

Пусть есть выборка $X_1,\ldots,X_n$ из неизвестного распределения $P\in\{P_\theta, \theta \in \Theta\}$. Размерность вектора параметров равна $k$, то есть $\Theta\in \mathbb{R}^k$. В таком случае понадобится $k$ пробных функций. Пусть $g_1(x),\ldots,g_k(x)$ — это некие борелевские функции такие, что они не являются линейно зависимыми. Пусть $m_i(\theta)=E_\theta g_i(X_1), i=1,\ldots, k$. Тогда **оценкой по методу моментов для параметров** $\theta$  является **решение** следующей системы, относительно $\theta^*$
$$\begin{cases}
m_1(\theta^*)=\overline{g_1(X)} \\
\vdots \\
m_k(\theta^*)=\overline{g_k(X)}
\end{cases}$$

## пример

Найти оценки методом моментов для нормального закона  $\mathcal{N}(a,\sigma^2)$ и для гамма распределения $\Gamma(\alpha, \beta)$.

Когда нет ограничений, можно использовать стандартные пробные функции $g_i(x)=x^i$.

Для нормального закона можно воспользоваться первой и второй стандартными пробными функциями.
$$g_1(x)=x;g_2(x)=x^2$$
тогда
$$m_1(\theta)=E_\theta g_1(X_1)=E_\theta X_1=a$$
$$m_2(\theta)=E_\theta g_2(X_1)=E_\theta X_1^2=\sigma^2 + a^2$$
составляем систему
$$\begin{cases}
a^*=\overline{X}\\
(a^*)^2+(\sigma^2)^*=\overline{X^2}
\end{cases}$$
тогда оценки
$$a^*=\overline{X}; (\sigma^2)^*=s^2$$

Для гамма распределения
$$m_1(\theta)=E_\theta X_1=\frac{\alpha}{\beta}$$
$$m_2(\theta)=E_\theta X_1^2=\frac{\alpha(\alpha+1)}{\beta^2}$$
составляем систему
$$\begin{cases}
\frac{\alpha^*}{\beta^*}=\overline{X} \\
\frac{\alpha^*(\alpha^*+1)}{(\beta^*)^2}=\overline{X^2}
\end{cases}$$
нужно возвести первое уравнение в квадрат и вычесть из второго
$$\frac{\alpha^*(\alpha^*+1)}{(\beta^*)^2}=\overline{X^2}-(\overline{X})^2=s^2$$
$$\alpha^*=\frac{(\overline{X})^2}{s^2};\beta^*=\frac{\overline{X}}{s^2}$$

## теорема

Пусть $m(\theta)=(m_1(\theta),\ldots,m_k(\theta))$ — многомерная функция. Пусть функция $m(\theta)$ — объективная, $m^{-1}(\theta)$  — непрерывная функция на $m(\Theta)$ и для любого $i$  математическое ожидание $E_\theta|g_i(X_1)|<+\infty$. В таком случае оценка по методу моментов является состоятельной для параметра $\theta$.

Если $m^{-1}(\theta)$ — непрервыно дифференцируема и для любого $i$ существует $E_\theta(g_i(X_1))^2<+\infty$, то оценка параметра $\theta$ также асимптотически нормальная.

# метод максимального правдоподобия

Функция $p_\theta(x)$ — обобщенная плотность для семейства распределений $\{P_\theta, \theta \in \Theta\}$ в следующих случаях:

1. если $\{P_\theta, \theta \in \Theta\}$ — это семейство абсолютно непрерывных распределений, то $p_\theta(x)$ — это плотность вероятностной меры 
$$p_\theta(x)=f(x,\theta)$$
где $f(x,\theta)$ — это плотность $P_\theta$

2. если все $P_\theta$ — дискретны, то 
$$p_\theta(x)=P_\theta(X=x)$$
где $X\sim P_\theta$

Пусть есть семейство вероятностных мер $\{P_\theta, \theta \in \Theta\}$ c [[обобщенная плотность|обобщенной плотностью]] $p_\theta(x)$. Пусть есть выборка $(X_1,\ldots,X_n)$ из неизвестного распределения $P$, которого принадлежит этому семейству $P_\theta$. Тогда **функцией правдоподобия** называется
$$f_\theta(X_1,\ldots,X_n)=\prod_{i=1}^np_\theta(X_i)$$
**Логарифмическая функция правдоподобия** 
$$L(X_1,\ldots,X_n,\theta)=\log f_\theta(X_1,\ldots,X_n)$$

Оценка по методу максимального правдоподобия (ОМП) определяется по следующему правилу
$$\theta^*(X)=\arg\max_\theta f_\theta(X_1,\ldots,X_n)$$
## пример

Найти методом ОМП оценку распределения $X_1, \ldots, X_n$ для $\theta$, если выборка из $\text{Bern}(\theta)$

Распределение Бернулли — это [[биномиальное распределение]], которое сосредоточено в двух значениях: 0 и 1.

Обобщенная плотность биномиального распределения
$$p_\theta(X)=\begin{cases}
\theta,&x=1\\
1-\theta,&x=0
\end{cases}=\theta^{I(x=1)}\cdot(1-\theta)^{I(x=0)}=$$
$$=\theta^x(1-\theta)^{1-x}$$
Функция правдоподобия биномиального распределения
$$f_\theta(X_1,\ldots,X_n)=\theta^{\sum X_i}(1-\theta)^{n-\sum X_i}$$
Надо найти максимум функции. Продифференцировать такую функцию будет сложно, поэтому сначала надо взять от нее логарифм и получится **уравнение правдоподобия**
$$\frac{\partial}{\partial \theta}L(X_1, \ldots, X_n)=0$$
Если у уравнения правдоподобия единственный корень, то этот корень является максимумом.
$$L(X_1,\ldots,X_n)=\sum X_i \log \theta + (n-\sum X_i)\log(1-\theta)$$
теперь надо продифференцировать
$$\frac{\partial}{\partial \theta}L(X,\theta)=\frac{\sum X_i}{\theta}-\frac{n-\sum X_i}{1-\theta}=0$$
решив это уравнение получаем, что оценка для параметра $\theta$
$$\theta^*=\overline{X}$$
## свойства оценки максимального правдоподобия

Пусть выполнены некоторые условия регулярности:
- у уравнения правдоподобия единственное решение
- носитель обощенной плотности $p_\theta(X)$ (там, где плотность отлична от нуля) не зависит от $\theta$
- функция правдоподобия дифференцируема по $\theta$
Тогда оценка методом максимального правдоподобия является состоятельной оценкой параметра $\theta$.

При некоторых дополнительных условиях регулярности:
- $\exists i\ne0, i(\theta)=E_\theta\big(\frac{\partial}{\partial\theta}\log p_\theta(X_1)\big)^2$ 
- третья производная по $\theta$ от обобщенной плотности должна быть ограничена некоей функцией $m(x)$, не зависящей от $\theta$ и интегрируемой
Тогда оценка методом максимального правдоподобия является асимтотически нормальной
$$\sqrt{n}(\hat{\theta}-\theta)\xrightarrow[]{d_\theta}\mathcal{N}\bigg(0, \frac{1}{i(\theta)}\bigg)$$

# метод выборочной квантили

Пусть есть выборка $X_1, \ldots, X_n$ и порядковые статистики $X_{(1)}\le\ldots\le X_{(n)}$, которые образуют вариационный ряд выборки. Тогда выборочной квантилью уровня $p$ называтся статистика
$$\widehat{Z}_p=\begin{cases}
X_{(np)}, & np\in \mathbb{N} \\
X_{([np]+1)}, & np\notin\mathbb{N}
\end{cases}$$

Медиана $\mu$ для функции распределения F, если $\mu$ определяется по следующему правилу:
$$\mu:=\inf\bigg\{x:F(x)\ge\frac{1}{2}\bigg\}$$
Если распределение непрерывно и монотонно, то $F(\mu)=1/2$.

Выборочная медиана
$$\tilde{\mu}=\begin{cases}
X_{(k+1)}, & n=2k+1 \\
\frac{X_{(k)}+X_{(k+1)}}{2}, &n=2k
\end{cases}$$

## теорема о выборочной квантили

Теорема выполнена только для распределений, у которых есть плотность, которая к тому же должна быть дифференцируема в требуемой точке.

Пусть есть выборка $X_1, \ldots, X_n$ с плотностью $f(x)$. Пусть $f(x)$ непрерывно дифференцируема в окрестности квантили $Z_p$ и $f(Z_p)>0$. Тогда выполнено предельное соотношение:
$$\sqrt{n}(\widehat{Z}_p-Z_p)\xrightarrow[n\to\infty]{d}\mathcal{N}\bigg(0,\frac{p(1-p)}{f^2(Z_p)}\bigg)$$

## теорема о выборочной медиане

В тех же условиях
$$\sqrt{n}(\tilde{\mu}-\mu)\xrightarrow[n\to\infty]{d}\mathcal{N}\bigg(0,\frac{1}{4f^2(\mu)}\bigg)$$

## применение

Пусть есть выборка $X_1, \ldots, X_n$ из распределения Коши со сдвигом, с плотностью 
$$f_\theta(x)=\frac{1}{\pi((x-\theta)^2+1)}$$
Найти оценку параметра $\theta$.

Распределение Коши не имеет математического ожидания.

$\theta$ — это медиана распределения, поэтому
$$\sqrt{n}(\tilde{\mu}-\mu)\xrightarrow[]{d_\theta}\mathcal{N}\bigg(0,\frac{\pi^2}{4}\bigg)$$